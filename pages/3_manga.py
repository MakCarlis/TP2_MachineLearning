# ============================================
# PARTIE 3 : EXP√âRIMENTATION SUR UN NOUVEAU JEU DE DONN√âES
# ============================================
# Dataset : Meilleures ventes de mangas (best-selling-manga.csv)
# Objectif : Pr√©dire les ventes totales en fonction des caract√©ristiques des mangas
# Probl√®me : R√©gression (ventes en millions)
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("PARTIE 3 : ANALYSE DES VENTES DE MANGAS")
print("=" * 80)

# ============================================
# 1. CHARGEMENT ET PREMI√àRE ANALYSE
# ============================================

print("\n" + "=" * 60)
print("1. CHARGEMENT ET ANALYSE EXPLORATOIRE")
print("=" * 60)

# Charger le dataset
df_manga = pd.read_csv('best-selling-manga.csv', encoding='utf-8')

print(f"\nüìä Dimensions du dataset : {df_manga.shape}")
print(f"   - {df_manga.shape[0]} mangas")
print(f"   - {df_manga.shape[1]} caract√©ristiques")

print("\nüìã Aper√ßu des 5 premi√®res lignes :")
print(df_manga.head())

print("\n‚ÑπÔ∏è Informations sur les types de donn√©es :")
print(df_manga.info())

print("\nüìà Statistiques descriptives :")
print(df_manga.describe())

# ============================================
# 2. NETTOYAGE ET PR√âPARATION DES DONN√âES
# ============================================

print("\n" + "=" * 60)
print("2. NETTOYAGE ET PR√âPARATION DES DONN√âES")
print("=" * 60)

# V√©rifier les valeurs manquantes
print("\nüîç Valeurs manquantes :")
missing_values = df_manga.isnull().sum()
print(missing_values[missing_values > 0])

# Traitement des valeurs manquantes
# Pour 'Demographic', on remplace par 'Unknown'
df_manga['Demographic'].fillna('Unknown', inplace=True)

# Pour les colonnes num√©riques, v√©rifier les valeurs aberrantes
print("\nüîç Analyse des valeurs aberrantes :")

# Examiner la colonne 'Approximate sales in million(s)'
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(df_manga['Approximate sales in million(s)'], bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Ventes (millions)')
plt.ylabel('Nombre de mangas')
plt.title('Distribution des ventes totales')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.boxplot(df_manga['Approximate sales in million(s)'])
plt.ylabel('Ventes (millions)')
plt.title('Boxplot des ventes totales')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('manga_sales_distribution.png', dpi=150)
plt.show()

# Statistiques sur les ventes
print("\nüìä Statistiques des ventes :")
print(f"   - Min : {df_manga['Approximate sales in million(s)'].min():.1f} millions")
print(f"   - Max : {df_manga['Approximate sales in million(s)'].max():.1f} millions")
print(f"   - Moyenne : {df_manga['Approximate sales in million(s)'].mean():.1f} millions")
print(f"   - M√©diane : {df_manga['Approximate sales in million(s)'].median():.1f} millions")
print(f"   - √âcart-type : {df_manga['Approximate sales in million(s)'].std():.1f} millions")

# Identifier les outliers (m√©thode IQR)
Q1 = df_manga['Approximate sales in million(s)'].quantile(0.25)
Q3 = df_manga['Approximate sales in million(s)'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df_manga[(df_manga['Approximate sales in million(s)'] < lower_bound) | 
                    (df_manga['Approximate sales in million(s)'] > upper_bound)]
print(f"\nüîç Nombre d'outliers d√©tect√©s : {len(outliers)}")
print("Top 5 des outliers (ventes exceptionnelles) :")
print(outliers[['Manga series', 'Approximate sales in million(s)']].head())

# ============================================
# 3. CR√âATION DE NOUVELLES FEATURES (FEATURE ENGINEERING)
# ============================================

print("\n" + "=" * 60)
print("3. CR√âATION DE NOUVELLES FEATURES")
print("=" * 60)

# Extraire l'ann√©e de d√©but de s√©rialisation
def extract_start_year(serialized):
    """Extrait l'ann√©e de d√©but de la cha√Æne 'ann√©e‚Äìann√©e'"""
    if pd.isna(serialized):
        return None
    try:
        # Format: "1997‚Äìpresent" ou "1984‚Äì1995" ou "1990‚Äìpresent"
        parts = str(serialized).split('‚Äì')
        if len(parts) > 0:
            year_str = parts[0].strip()
            if year_str.isdigit() and len(year_str) == 4:
                return int(year_str)
    except:
        pass
    return None

def extract_end_year(serialized):
    """Extrait l'ann√©e de fin de la cha√Æne"""
    if pd.isna(serialized):
        return None
    try:
        parts = str(serialized).split('‚Äì')
        if len(parts) > 1:
            year_str = parts[1].strip()
            if year_str == 'present':
                return 2024  # Ann√©e actuelle pour les s√©ries en cours
            elif year_str.isdigit() and len(year_str) == 4:
                return int(year_str)
    except:
        pass
    return None

print("\nüîß Extraction des ann√©es de publication...")
df_manga['start_year'] = df_manga['Serialized'].apply(extract_start_year)
df_manga['end_year'] = df_manga['Serialized'].apply(extract_end_year)

# Calculer la dur√©e de publication
df_manga['publication_years'] = df_manga['end_year'] - df_manga['start_year']

# V√©rifier les r√©sultats
print("\nüìä Ann√©es extraites (√©chantillon) :")
print(df_manga[['Manga series', 'Serialized', 'start_year', 'end_year', 'publication_years']].head(10))

# Statistiques sur la dur√©e
print("\nüìà Statistiques de la dur√©e de publication :")
print(f"   - Dur√©e moyenne : {df_manga['publication_years'].mean():.1f} ans")
print(f"   - Dur√©e m√©diane : {df_manga['publication_years'].median():.1f} ans")
print(f"   - Dur√©e max : {df_manga['publication_years'].max()} ans (Golgo 13, depuis 1968!)")

# Cr√©er une feature pour les s√©ries en cours
df_manga['ongoing'] = df_manga['Serialized'].str.contains('present', na=False).astype(int)

# Calculer les ventes par volume (si pas d√©j√† pr√©sent)
df_manga['sales_per_volume'] = df_manga['Approximate sales in million(s)'] / df_manga['No. of collected volumes']

print("\nüìä Nouvelles features cr√©√©es :")
print(f"   - start_year : ann√©e de d√©but")
print(f"   - end_year : ann√©e de fin")
print(f"   - publication_years : dur√©e de publication")
print(f"   - ongoing : 1 si s√©rie en cours, 0 sinon")
print(f"   - sales_per_volume : ventes moyennes par volume")

# ============================================
# 4. ANALYSE EXPLORATOIRE APPROFONDIE
# ============================================

print("\n" + "=" * 60)
print("4. ANALYSE EXPLORATOIRE APPROFONDIE")
print("=" * 60)

# Analyser les ventes par d√©mographie
print("\nüìä Ventes par d√©mographie :")
demographic_stats = df_manga.groupby('Demographic')['Approximate sales in million(s)'].agg(['count', 'mean', 'median', 'sum']).sort_values('sum', ascending=False)
print(demographic_stats)

# Visualisation
plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
demographic_stats['sum'].plot(kind='bar', color='skyblue')
plt.title('Ventes totales par d√©mographie')
plt.xlabel('D√©mographie')
plt.ylabel('Ventes totales (millions)')
plt.xticks(rotation=45)

plt.subplot(2, 3, 2)
demographic_stats['mean'].plot(kind='bar', color='lightgreen')
plt.title('Ventes moyennes par d√©mographie')
plt.xlabel('D√©mographie')
plt.ylabel('Ventes moyennes (millions)')
plt.xticks(rotation=45)

# Analyser les ventes par √©diteur
print("\nüìä Top 10 √©diteurs par ventes totales :")
publisher_stats = df_manga.groupby('Publisher')['Approximate sales in million(s)'].sum().sort_values(ascending=False).head(10)
print(publisher_stats)

plt.subplot(2, 3, 3)
publisher_stats.plot(kind='bar', color='coral')
plt.title('Top 10 √©diteurs par ventes')
plt.xlabel('√âditeur')
plt.ylabel('Ventes totales (millions)')
plt.xticks(rotation=45)

# Relation entre nombre de volumes et ventes
plt.subplot(2, 3, 4)
plt.scatter(df_manga['No. of collected volumes'], df_manga['Approximate sales in million(s)'], alpha=0.6)
plt.xlabel('Nombre de volumes')
plt.ylabel('Ventes totales (millions)')
plt.title('Ventes vs Nombre de volumes')
plt.grid(True, alpha=0.3)

# Calculer la corr√©lation
corr_volumes_sales = df_manga['No. of collected volumes'].corr(df_manga['Approximate sales in million(s)'])
print(f"\nüìà Corr√©lation volumes-ventes : {corr_volumes_sales:.3f}")

# Relation entre dur√©e et ventes
plt.subplot(2, 3, 5)
plt.scatter(df_manga['publication_years'], df_manga['Approximate sales in million(s)'], alpha=0.6)
plt.xlabel('Dur√©e de publication (ann√©es)')
plt.ylabel('Ventes totales (millions)')
plt.title('Ventes vs Dur√©e de publication')
plt.grid(True, alpha=0.3)

corr_years_sales = df_manga['publication_years'].corr(df_manga['Approximate sales in million(s)'])
print(f"üìà Corr√©lation dur√©e-ventes : {corr_years_sales:.3f}")

# Distribution des ventes par volume
plt.subplot(2, 3, 6)
plt.hist(df_manga['sales_per_volume'].dropna(), bins=30, edgecolor='black', alpha=0.7)
plt.xlabel('Ventes moyennes par volume (millions)')
plt.ylabel('Fr√©quence')
plt.title('Distribution des ventes par volume')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('manga_detailed_analysis.png', dpi=150)
plt.show()

# ============================================
# 5. MATRICE DE CORR√âLATION
# ============================================

print("\n" + "=" * 60)
print("5. MATRICE DE CORR√âLATION")
print("=" * 60)

# S√©lectionner les colonnes num√©riques pour la corr√©lation
numeric_cols = ['No. of collected volumes', 'Approximate sales in million(s)', 
                'Average sales per volume in million(s)', 'start_year', 
                'publication_years', 'sales_per_volume', 'ongoing']

# Filtrer les colonnes existantes
available_numeric = [col for col in numeric_cols if col in df_manga.columns]
corr_matrix = df_manga[available_numeric].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=1, fmt='.3f')
plt.title('Matrice de corr√©lation - Variables num√©riques')
plt.tight_layout()
plt.savefig('manga_correlation_matrix.png', dpi=150)
plt.show()

print("\nüîç Corr√©lations avec les ventes totales :")
corr_with_sales = corr_matrix['Approximate sales in million(s)'].sort_values(ascending=False)
print(corr_with_sales)

# ============================================
# 6. PR√âPARATION POUR LE MACHINE LEARNING
# ============================================

print("\n" + "=" * 60)
print("6. PR√âPARATION POUR LE MACHINE LEARNING")
print("=" * 60)

# D√©finir les features et la cible
# Objectif : pr√©dire 'Approximate sales in million(s)' (ventes totales)
target = 'Approximate sales in million(s)'

# S√©lectionner les features pertinentes
feature_cols = [
    'No. of collected volumes',
    'publication_years',
    'ongoing',
    'start_year'
]

# Ajouter les variables cat√©gorielles
categorical_cols = ['Demographic', 'Publisher']

print(f"\nüéØ Variable cible : {target}")
print(f"\nüìä Features num√©riques : {feature_cols}")
print(f"üìä Features cat√©gorielles : {categorical_cols}")

# Cr√©er X et y
X = df_manga[feature_cols + categorical_cols].copy()
y = df_manga[target].copy()

# V√©rifier les valeurs manquantes
print("\nüîç Valeurs manquantes dans X :")
print(X.isnull().sum())

# Supprimer les lignes avec des valeurs manquantes
X = X.dropna()
y = y[X.index]  # Aligner y avec X

print(f"\nüìè Dimensions apr√®s nettoyage : X={X.shape}, y={y.shape}")

# ============================================
# 7. CR√âATION DU PIPELINE DE PR√âPROCESSING
# ============================================

print("\n" + "=" * 60)
print("7. CR√âATION DU PIPELINE DE PR√âPROCESSING")
print("=" * 60)

# D√©finir les transformations pour les colonnes num√©riques
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# D√©finir les transformations pour les colonnes cat√©gorielles
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

# Combiner les transformations
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, feature_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

print("‚úì Pipeline de preprocessing cr√©√©")
print("   - Normalisation des features num√©riques")
print("   - One-hot encoding des variables cat√©gorielles")

# ============================================
# 8. SPLIT TRAIN/TEST
# ============================================

print("\n" + "=" * 60)
print("8. SPLIT TRAIN/TEST")
print("=" * 60)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nüìä Taille train : {X_train.shape[0]} ({len(X_train)/len(X):.1%})")
print(f"üìä Taille test : {X_test.shape[0]} ({len(X_test)/len(X):.1%})")

print(f"\nüìà Statistiques de y_train :")
print(f"   - Min : {y_train.min():.1f}")
print(f"   - Max : {y_train.max():.1f}")
print(f"   - Moyenne : {y_train.mean():.1f}")
print(f"   - M√©diane : {y_train.median():.1f}")

# ============================================
# 9. ENTRA√éNEMENT DE PLUSIEURS MOD√àLES
# ============================================

print("\n" + "=" * 60)
print("9. ENTRA√éNEMENT DE PLUSIEURS MOD√àLES")
print("=" * 60)

# Dictionnaire des mod√®les √† tester
models = {
    'R√©gression Lin√©aire': LinearRegression(),
    'Ridge (L2)': Ridge(alpha=1.0),
    'Lasso (L1)': Lasso(alpha=0.01),
    'KNN': KNeighborsRegressor(n_neighbors=5),
    'Arbre de d√©cision': DecisionTreeRegressor(max_depth=5, random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(kernel='rbf')
}

results = []

print("\nüèãÔ∏è Entra√Ænement des mod√®les en cours...")
for name, model in models.items():
    # Cr√©er le pipeline complet
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    
    # Entra√Ænement
    pipeline.fit(X_train, y_train)
    
    # Pr√©dictions
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    # M√©triques
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    results.append({
        'Mod√®le': name,
        'Train MAE': train_mae,
        'Test MAE': test_mae,
        'Train R¬≤': train_r2,
        'Test R¬≤': test_r2,
        '√âcart MAE': test_mae - train_mae,
        'Pipeline': pipeline  # Sauvegarder pour usage ult√©rieur
    })
    
    print(f"\n{name}:")
    print(f"   MAE Test: {test_mae:.2f}M, R¬≤ Test: {test_r2:.3f}")

# Cr√©er un DataFrame pour comparer
results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'Pipeline'} for r in results])
results_df = results_df.sort_values('Test MAE')

print("\n" + "=" * 60)
print("üìä TABLEAU COMPARATIF DES MOD√àLES")
print("=" * 60)
print(results_df.to_string(index=False))

# ============================================
# 10. ANALYSE DES R√âSULTATS
# ============================================

print("\n" + "=" * 60)
print("10. ANALYSE DES R√âSULTATS")
print("=" * 60)

best_model_row = results_df.iloc[0]
best_model_name = best_model_row['Mod√®le']
best_mae = best_model_row['Test MAE']
best_r2 = best_model_row['Test R¬≤']

print(f"\nüèÜ Meilleur mod√®le : {best_model_name}")
print(f"   - MAE : {best_mae:.2f} millions")
print(f"   - R¬≤  : {best_r2:.3f} ({best_r2*100:.1f}%)")

print(f"\nüìà Interpr√©tation des m√©triques :")
print(f"   - MAE (Mean Absolute Error) : En moyenne, le mod√®le se trompe de {best_mae:.2f} millions")
print(f"     dans ses pr√©dictions de ventes.")
print(f"   - R¬≤ (Coefficient de d√©termination) : Le mod√®le explique {best_r2*100:.1f}%")
print(f"     de la variance des ventes. C'est {'excellent' if best_r2 > 0.7 else 'bon' if best_r2 > 0.5 else 'moyen'}.")

# Comparaison avec un mod√®le na√Øf (pr√©dire la moyenne)
naive_pred = np.full_like(y_test, y_train.mean())
naive_mae = mean_absolute_error(y_test, naive_pred)
naive_r2 = r2_score(y_test, naive_pred)

print(f"\nüìä Comparaison avec un mod√®le na√Øf (pr√©dire la moyenne) :")
print(f"   - Mod√®le na√Øf MAE : {naive_mae:.2f}M")
print(f"   - {best_model_name} MAE : {best_mae:.2f}M")
print(f"   - Am√©lioration : {(naive_mae - best_mae)/naive_mae*100:.1f}%")

# ============================================
# 11. R√âCUP√âRATION DU PIPELINE DU MEILLEUR MOD√àLE
# ============================================

# R√©cup√©rer le pipeline du meilleur mod√®le
best_pipeline = None
for r in results:
    if r['Mod√®le'] == best_model_name:
        best_pipeline = r['Pipeline']
        break

# ============================================
# 12. ANALYSE DES R√âSIDUS
# ============================================

print("\n" + "=" * 60)
print("11. ANALYSE DES R√âSIDUS")
print("=" * 60)

if best_pipeline:
    # Pr√©dictions finales
    y_test_pred = best_pipeline.predict(X_test)
    residuals = y_test - y_test_pred
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(y_test_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.xlabel('Valeurs pr√©dites (millions)')
    plt.ylabel('R√©sidus')
    plt.title(f'R√©sidus vs Pr√©dictions - {best_model_name}')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.hist(residuals, bins=20, edgecolor='black', alpha=0.7)
    plt.xlabel('R√©sidus')
    plt.ylabel('Fr√©quence')
    plt.title('Distribution des r√©sidus')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('manga_residuals.png', dpi=150)
    plt.show()
    
    print(f"\nüìä Statistiques des r√©sidus :")
    print(f"   - Moyenne : {residuals.mean():.4f} (devrait √™tre proche de 0)")
    print(f"   - √âcart-type : {residuals.std():.4f}")
    print(f"   - Min : {residuals.min():.4f}")
    print(f"   - Max : {residuals.max():.4f}")
    
    # Test de normalit√© (Shapiro-Wilk)
    from scipy import stats
    shapiro_stat, shapiro_p = stats.shapiro(residuals[:100])  # Limit to 100 for speed
    print(f"\nüìä Test de normalit√© de Shapiro-Wilk :")
    print(f"   - Statistique : {shapiro_stat:.4f}")
    print(f"   - p-value : {shapiro_p:.4f}")
    if shapiro_p > 0.05:
        print("   ‚úÖ Les r√©sidus suivent une distribution normale")
    else:
        print("   ‚ö† Les r√©sidus ne suivent pas une distribution normale")

# ============================================
# 13. IMPORTANCE DES VARIABLES (SI DISPONIBLE)
# ============================================

print("\n" + "=" * 60)
print("12. IMPORTANCE DES VARIABLES")
print("=" * 60)

if best_pipeline and hasattr(best_pipeline.named_steps['regressor'], 'feature_importances_'):
    # R√©cup√©rer les noms des features apr√®s preprocessing
    feature_names = []
    feature_names.extend(feature_cols)
    
    # Ajouter les noms des variables one-hot
    cat_encoder = best_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot']
    cat_feature_names = cat_encoder.get_feature_names_out(categorical_cols)
    feature_names.extend(cat_feature_names)
    
    # R√©cup√©rer les importances
    importances = best_pipeline.named_steps['regressor'].feature_importances_
    
    # Cr√©er un DataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names[:len(importances)],
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    print(f"\nüîç Top 10 features les plus importantes ({best_model_name}) :")
    print(importance_df.head(10).to_string(index=False))
    
    # Visualisation
    plt.figure(figsize=(10, 8))
    plt.barh(importance_df.head(15)['feature'][::-1], 
             importance_df.head(15)['importance'][::-1])
    plt.xlabel('Importance')
    plt.title(f'Importance des variables - {best_model_name}')
    plt.tight_layout()
    plt.savefig('manga_feature_importance.png', dpi=150)
    plt.show()
    
    print("\nüìä Interpr√©tation des features importantes :")
    top_features = importance_df.head(5)
    for idx, row in top_features.iterrows():
        print(f"   - {row['feature']} : {row['importance']:.3f}")
        if 'No. of collected volumes' in row['feature']:
            print("     ‚Üí Le nombre de volumes est un pr√©dicteur naturel des ventes totales")
        elif 'publication_years' in row['feature']:
            print("     ‚Üí Les s√©ries longues accumulent plus de ventes")
        elif 'Demographic' in row['feature']:
            demo = row['feature'].replace('Demographic_', '')
            print(f"     ‚Üí La d√©mographie {demo} est surrepr√©sent√©e dans les best-sellers")

# ============================================
# 14. VALIDATION CROIS√âE
# ============================================

print("\n" + "=" * 60)
print("13. VALIDATION CROIS√âE")
print("=" * 60)

if best_pipeline:
    # Validation crois√©e 5-fold
    cv_scores_mae = cross_val_score(best_pipeline, X, y, cv=5, 
                                     scoring='neg_mean_absolute_error')
    cv_scores_r2 = cross_val_score(best_pipeline, X, y, cv=5, 
                                    scoring='r2')
    
    print(f"\nüìä Validation crois√©e 5-fold pour {best_model_name} :")
    print(f"   - MAE moyen : {-cv_scores_mae.mean():.4f} ¬± {cv_scores_mae.std():.4f}")
    print(f"   - R¬≤ moyen : {cv_scores_r2.mean():.4f} ¬± {cv_scores_r2.std():.4f}")
    
    print(f"\nüìà Interpr√©tation :")
    print(f"   - L'√©cart-type faible ({cv_scores_r2.std():.4f}) indique que le mod√®le est stable")
    print(f"   - Le mod√®le g√©n√©ralise bien √† diff√©rents √©chantillons")

# ============================================
# 15. OPTIMISATION DU MEILLEUR MOD√àLE
# ============================================

print("\n" + "=" * 60)
print("14. OPTIMISATION DU MEILLEUR MOD√àLE")
print("=" * 60)

if best_model_name == 'Random Forest':
    # GridSearch pour Random Forest
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__max_depth': [5, 10, 15, None],
        'regressor__min_samples_split': [2, 5, 10],
        'regressor__min_samples_leaf': [1, 2, 4]
    }
    
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(random_state=42))
    ])
    
    print("\nüîç Recherche des meilleurs hyperparam√®tres...")
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, 
                               scoring='neg_mean_absolute_error',
                               n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    
    print(f"\n‚úÖ Meilleurs param√®tres trouv√©s :")
    for param, value in grid_search.best_params_.items():
        print(f"   - {param} : {value}")
    
    best_model_optimized = grid_search.best_estimator_
    
    # √âvaluation
    y_test_pred_opt = best_model_optimized.predict(X_test)
    opt_mae = mean_absolute_error(y_test, y_test_pred_opt)
    opt_r2 = r2_score(y_test, y_test_pred_opt)
    
    print(f"\nüìä Performance apr√®s optimisation :")
    print(f"   - MAE : {opt_mae:.4f} (vs {best_mae:.4f})")
    print(f"   - R¬≤ : {opt_r2:.4f} (vs {best_r2:.4f})")
    print(f"   - Am√©lioration MAE : {(best_mae - opt_mae)/best_mae*100:.1f}%")
    
    best_final_model = best_model_optimized
    final_mae = opt_mae
    final_r2 = opt_r2
    
elif best_model_name == 'Gradient Boosting':
    param_grid = {
        'regressor__n_estimators': [50, 100, 200],
        'regressor__learning_rate': [0.05, 0.1, 0.2],
        'regressor__max_depth': [3, 4, 5],
        'regressor__subsample': [0.8, 1.0]
    }
    
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', GradientBoostingRegressor(random_state=42))
    ])
    
    print("\nüîç Recherche des meilleurs hyperparam√®tres...")
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, 
                               scoring='neg_mean_absolute_error',
                               n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    
    print(f"\n‚úÖ Meilleurs param√®tres trouv√©s :")
    for param, value in grid_search.best_params_.items():
        print(f"   - {param} : {value}")
    
    best_model_optimized = grid_search.best_estimator_
    
    # √âvaluation
    y_test_pred_opt = best_model_optimized.predict(X_test)
    opt_mae = mean_absolute_error(y_test, y_test_pred_opt)
    opt_r2 = r2_score(y_test, y_test_pred_opt)
    
    print(f"\nüìä Performance apr√®s optimisation :")
    print(f"   - MAE : {opt_mae:.4f} (vs {best_mae:.4f})")
    print(f"   - R¬≤ : {opt_r2:.4f} (vs {best_r2:.4f})")
    print(f"   - Am√©lioration MAE : {(best_mae - opt_mae)/best_mae*100:.1f}%")
    
    best_final_model = best_model_optimized
    final_mae = opt_mae
    final_r2 = opt_r2
    
else:
    best_final_model = best_pipeline
    final_mae = best_mae
    final_r2 = best_r2

# ============================================
# 16. PR√âDICTIONS SUR DES CAS CONCRETS
# ============================================

print("\n" + "=" * 60)
print("15. PR√âDICTIONS SUR DES CAS CONCRETS")
print("=" * 60)

# Cr√©er quelques exemples de mangas pour tester le mod√®le
test_cases = pd.DataFrame({
    'No. of collected volumes': [30, 50, 100, 20],
    'publication_years': [5, 15, 30, 3],
    'ongoing': [0, 1, 1, 0],
    'start_year': [2018, 2010, 1995, 2020],
    'Demographic': ['Sh≈çnen', 'Seinen', 'Sh≈çnen', 'Sh≈çjo'],
    'Publisher': ['Shueisha', 'Kodansha', 'Shogakukan', 'Hakusensha']
})

if best_final_model:
    predictions = best_final_model.predict(test_cases)
    
    print("\nüîÆ Pr√©dictions pour des mangas hypoth√©tiques :")
    for i, pred in enumerate(predictions):
        print(f"\nCas {i+1}: {test_cases.iloc[i].to_dict()}")
        print(f"   ‚Üí Ventes pr√©dites : {pred:.1f} millions")
        
        # Interpr√©tation
        if pred > 50:
            print("     ‚≠ê Potentiel best-seller majeur")
        elif pred > 20:
            print("     üìà Bonnes ventes attendues")
        else:
            print("     üìä Ventes modestes")

# ============================================
# 17. SAUVEGARDE DU MOD√àLE
# ============================================

print("\n" + "=" * 60)
print("16. SAUVEGARDE DU MOD√àLE")
print("=" * 60)

import pickle

if best_final_model:
    with open('manga_sales_model.pkl', 'wb') as f:
        pickle.dump(best_final_model, f)
    
    print("‚úÖ Mod√®le sauvegard√© sous 'manga_sales_model.pkl'")
    print("   - Pour pr√©dire les ventes de nouveaux mangas")
    print("   - Utiliser pipeline.predict(nouveaux_donn√©es)")

# ============================================
# 18. CONCLUSION G√âN√âRALE
# ============================================

print("\n" + "=" * 80)
print("CONCLUSION G√âN√âRALE - ANALYSE DES VENTES DE MANGAS")
print("=" * 80)

print(f"""
üìö R√âSUM√â DE L'ANALYSE :

1. Dataset analys√© : {df_manga.shape[0]} mangas best-sellers

2. Caract√©ristiques cl√©s identifi√©es :
   - Le nombre de volumes est fortement corr√©l√© aux ventes
   - La dur√©e de publication est un facteur important
   - Sh≈çnen et Seinen dominent les ventes
   - Shueisha et Kodansha sont les √©diteurs majeurs

3. Mod√®le optimal : {best_model_name}
   - MAE : {final_mae:.2f} millions
   - R¬≤ : {final_r2:.2%}
   
4. Interpr√©tation m√©tier :
   - Une erreur moyenne de {final_mae:.2f}M est acceptable pour pr√©dire des ventes
   - Le mod√®le explique {final_r2:.1f}% des variations de ventes
   - Pour un nouveau manga avec 20 volumes sur 5 ans, on pr√©dit environ {predictions[3]:.1f}M

5. Limitations :
   - Donn√©es limit√©es aux best-sellers (biais de s√©lection)
   - Pas de donn√©es sur les adaptations (anime, films)
   - Pas de donn√©es sur les prix ou le marketing

6. Applications potentielles :
   - Aider les √©diteurs √† estimer le potentiel d'un nouveau manga
   - Identifier les facteurs de succ√®s
   - Comparer les performances par d√©mographie/√©diteur
""")

# ============================================
# 19. BONUS : VISUALISATION FINALE
# ============================================

print("\n" + "=" * 60)
print("BONUS : VISUALISATION FINALE")
print("=" * 60)

# Cr√©er un graphique r√©capitulatif
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Graphique 1 : Top 10 des mangas par ventes
top10 = df_manga.nlargest(10, 'Approximate sales in million(s)')
axes[0, 0].barh(top10['Manga series'], top10['Approximate sales in million(s)'])
axes[0, 0].set_xlabel('Ventes (millions)')
axes[0, 0].set_title('Top 10 des mangas les plus vendus')
axes[0, 0].invert_yaxis()

# Graphique 2 : Ventes par d√©mographie
demographic_means = df_manga.groupby('Demographic')['Approximate sales in million(s)'].mean().sort_values()
axes[0, 1].barh(demographic_means.index, demographic_means.values)
axes[0, 1].set_xlabel('Ventes moyennes (millions)')
axes[0, 1].set_title('Ventes moyennes par d√©mographie')

# Graphique 3 : Relation volumes-ventes
axes[1, 0].scatter(df_manga['No. of collected volumes'], 
                   df_manga['Approximate sales in million(s)'], alpha=0.5)
axes[1, 0].set_xlabel('Nombre de volumes')
axes[1, 0].set_ylabel('Ventes (millions)')
axes[1, 0].set_title('Relation volumes vs ventes')
axes[1, 0].grid(True, alpha=0.3)

# Graphique 4 : Pr√©dictions vs r√©alit√©
if best_final_model:
    y_all_pred = best_final_model.predict(X)
    axes[1, 1].scatter(y, y_all_pred, alpha=0.5)
    axes[1, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
    axes[1, 1].set_xlabel('Ventes r√©elles (millions)')
    axes[1, 1].set_ylabel('Ventes pr√©dites (millions)')
    axes[1, 1].set_title(f'Pr√©dictions vs r√©alit√© - {best_model_name}')
    axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('manga_final_summary.png', dpi=150)
plt.show()

print("\n‚úÖ Analyse termin√©e ! Tous les graphiques ont √©t√© sauvegard√©s.")