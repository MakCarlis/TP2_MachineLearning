# ============================================
# PARTIE 1 : MISE EN ≈íUVRE DES M√âTHODES D'ENSEMBLE
# ============================================

# Importation des biblioth√®ques n√©cessaires

import pandas as pd                  # Pour la manipulation des donn√©es
import numpy as np                    # Pour les calculs num√©riques
import matplotlib.pyplot as plt        # Pour les graphiques de base
import seaborn as sns                  # Pour des graphiques plus sophistiqu√©s
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score, 
                             roc_curve, auc)
import warnings
warnings.filterwarnings('ignore')      # Pour ignorer les avertissements non critiques
import pickle



# ============================================
# A : DESCRIPTION ET VISUALISATION DES DONN√âES
# ============================================

# Chargement du dataset
df = pd.read_csv('acs2017_county_data.csv')
print("=" * 60)
print("APER√áU DU DATASET")
print("=" * 60)

# 1) Utilisation des fonctions pandas pour l'analyse initiale

# shape : donne les dimensions (lignes, colonnes) du dataframe
print(f"\n1. Dimensions du dataset (shape) : {df.shape}")
print(f"   - {df.shape[0]} instances (comt√©s)")
print(f"   - {df.shape[1]} caract√©ristiques (variables)")

# info() : r√©sum√© des donn√©es (types, valeurs non-nulles)
print("\n2. Informations g√©n√©rales sur les donn√©es :")
print(df.info())

# head() : affiche les 5 premi√®res lignes pour voir la structure
print("\n3. Aper√ßu des 5 premi√®res lignes :")
print(df.head())

# describe() : statistiques descriptives pour les colonnes num√©riques
print("\n4. Statistiques descriptives :")
print(df.describe())

# R√©ponses aux questions sp√©cifiques
print("\n" + "=" * 60)
print("R√âPONSES AUX QUESTIONS DE LA PARTIE 1-A")
print("=" * 60)

# Combien de classes ? (pour la variable cible "Income")
# Note : Dans ce dataset, "Income" est le revenu moyen, pas une classe binaire
# On va cr√©er une variable cible binaire pour la classification
# On suppose que le seuil est 50K comme dans l'√©nonc√©
median_income = df['Income'].median()
df['Income_class'] = (df['Income'] > 50000).astype(int)
# 1 pour >50K, 0 pour <=50K

print(f"\na) Nombre de classes pour la variable cible 'Income' : 2 classes")
print(f"   - 0 : Revenu ‚â§ 50K ({(df['Income_class'] == 0).sum()} instances)")
print(f"   - 1 : Revenu > 50K ({(df['Income_class'] == 1).sum()} instances)")

# Combien de caract√©ristiques descriptives ? De quels types ?
# On exclut les colonnes d'identification et la cible
feature_cols = [col for col in df.columns if col not in ['CountyId', 'State', 'County', 'Income_class']]
print(f"\nb) Nombre de caract√©ristiques descriptives : {len(feature_cols)}")
print("\n   Types des caract√©ristiques :")
print(df[feature_cols].dtypes.value_counts())

# Combien d'instances ?
print(f"\nc) Nombre total d'instances : {df.shape[0]}")

# Combien d'instances de chaque classe ?
print(f"\nd) Distribution des classes :")
print(df['Income_class'].value_counts())
print(f"   En pourcentage :")
print(df['Income_class'].value_counts(normalize=True) * 100)

# Comment sont organis√©s les instances ?
print(f"\ne) Organisation des instances :")
print(f"   - Les donn√©es sont organis√©es par √âtat (State) et par comt√© (County)")
print(f"   - Nombre d'√âtats diff√©rents : {df['State'].nunique()}")
print(f"   - Nombre de comt√©s par √âtat (moyenne) : {df.groupby('State').size().mean():.1f}")

# ============================================
# 2) Visualisation des donn√©es : croisement des variables
# ============================================

print("\n" + "=" * 60)
print("VISUALISATION DES DONN√âES")
print("=" * 60)

# S√©lectionner quelques variables cl√©s pour la visualisation
# Pour √©viter d'avoir trop de graphiques, on choisit les plus pertinentes
key_vars = ['Income', 'TotalPop', 'Professional', 'Unemployment', 'Poverty']

# Cr√©er une copie avec seulement ces variables + la classe
df_viz = df[key_vars + ['Income_class']].copy()

# Convertir Income_class en cat√©gorie pour la couleur
df_viz['Income_class'] = df_viz['Income_class'].map({0: '‚â§50K', 1: '>50K'})

# 2a) Pairplot : matrice de scatter plots
print("\nCr√©ation du pairplot (cela peut prendre quelques secondes)...")
plt.figure(figsize=(12, 10))
pairplot = sns.pairplot(df_viz, hue='Income_class', diag_kind='hist', 
                        plot_kws={'alpha': 0.5, 's': 10})
pairplot.fig.suptitle("Matrice de dispersion des variables cl√©s", y=1.02)
plt.tight_layout()
plt.savefig('pairplot.png', dpi=150, bbox_inches='tight')
plt.show()
print("‚úì Graphique sauvegard√© sous 'pairplot.png'")

# 2b) Tracer les droites de r√©gression et donner les param√®tres
print("\nAnalyse des relations lin√©aires :")

# Fonction pour tracer une r√©gression lin√©aire
def plot_regression_with_params(x, y, data, title):
    plt.figure(figsize=(8, 6))
    
    # Nuage de points avec couleur selon la classe
    colors = {0: 'blue', 1: 'red'}
    for cls in [0, 1]:
        subset = data[data['Income_class'] == cls]
        plt.scatter(subset[x], subset[y], 
                   c=colors[cls], label=f'Classe {cls}', alpha=0.5, s=10)
    
    # Droite de r√©gression globale
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(data[x], data[y])
    
    x_range = np.linspace(data[x].min(), data[x].max(), 100)
    plt.plot(x_range, intercept + slope * x_range, 
            'k-', label=f'R√©gression lin√©aire', linewidth=2)
    
    plt.xlabel(x)
    plt.ylabel(y)
    plt.title(title)
    plt.legend()
    
    # Ajouter les param√®tres sous forme de texte
    text = f"√âquation : {y} = {slope:.4f} √ó {x} + {intercept:.2f}\n"
    text += f"R¬≤ = {r_value**2:.4f} | p-value = {p_value:.4f}"
    plt.text(0.05, 0.95, text, transform=plt.gca().transAxes,
            fontsize=10, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(f'regression_{x}_{y}.png', dpi=150)
    plt.show()
    
    return slope, intercept, r_value**2

# Analyser quelques relations importantes
relations = [
    ('Professional', 'Income', "Relation entre % de professionnels et revenu"),
    ('Unemployment', 'Poverty', "Relation entre ch√¥mage et pauvret√©"),
    ('TotalPop', 'Income', "Relation entre population et revenu")
]

results = []
for x, y, title in relations:
    print(f"\n--- {title} ---")
    slope, intercept, r2 = plot_regression_with_params(x, y, df, title)
    results.append({
        'x': x, 'y': y,
        'slope': slope, 'intercept': intercept,
        'r2': r2
    })

# Afficher un tableau r√©capitulatif
print("\n" + "=" * 60)
print("R√âCAPITULATIF DES R√âGRESSIONS LIN√âAIRES")
print("=" * 60)
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

print("\n" + "=" * 60)
print("COMMENTAIRES SUR LES R√âSULTATS")
print("=" * 60)
print("""
Observations principales :
1. Relation Professional vs Income :
   - Corr√©lation positive forte : plus le pourcentage de professionnels est √©lev√©,
     plus le revenu moyen est √©lev√©. C'est logique car les professions lib√©rales/
     techniques sont g√©n√©ralement mieux r√©mun√©r√©es.

2. Relation Unemployment vs Poverty :
   - Corr√©lation positive : le ch√¥mage est fortement li√© √† la pauvret√©.
     Les comt√©s avec un fort taux de ch√¥mage ont aussi un fort taux de pauvret√©.

3. Relation TotalPop vs Income :
   - Corr√©lation faible : la taille de la population n'est pas un bon pr√©dicteur
     du revenu moyen. Certaines grandes villes ont des revenus √©lev√©s mais pas toutes.

Ces observations nous aideront pour la suite : les variables comme 'Professional'
et 'Unemployment' seront probablement importantes pour la pr√©diction.
""")

# Sauvegarder le dataframe avec la nouvelle colonne pour la suite
df.to_csv('census_with_class.csv', index=False)
print("\n‚úì Dataset enrichi sauvegard√© sous 'census_with_class.csv'")


# ============================================
# S√âPARATION DES DONN√âES EN BASES D'APPRENTISSAGE ET DE TEST
# ============================================

print("=" * 60)
print("PR√âPARATION DES DONN√âES POUR LE MOD√àLE KNN")
print("=" * 60)

# Recharger le dataset avec la classe cr√©√©e pr√©c√©demment
df = pd.read_csv('census_with_class.csv')

# 1) Identifier les colonnes pertinentes
# On exclut les colonnes d'identification et la cible
feature_cols = [col for col in df.columns if col not in ['CountyId', 'State', 'County', 'Income', 'Income_class']]
print(f"\nNombre de caract√©ristiques initiales : {len(feature_cols)}")

# 2) G√©rer les valeurs manquantes (s'il y en a)
print("\nV√©rification des valeurs manquantes :")
missing_values = df[feature_cols].isnull().sum()
if missing_values.sum() > 0:
    print(f"  - {missing_values.sum()} valeurs manquantes d√©tect√©es")
    # Pour simplifier, on remplit les valeurs manquantes avec la m√©diane
    for col in feature_cols:
        if df[col].isnull().any():
            df[col].fillna(df[col].median(), inplace=True)
    print("  ‚úì Valeurs manquantes trait√©es")
else:
    print("  ‚úì Aucune valeur manquante")

# 3) Encoder les variables cat√©gorielles (si n√©cessaire)
# Dans ce dataset, toutes les variables semblent num√©riques
print("\nTypes des caract√©ristiques :")
print(df[feature_cols].dtypes.value_counts())

# 4) Normalisation des donn√©es (cruciale pour KNN)
# KNN est sensible aux √©chelles des variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[feature_cols])
X = pd.DataFrame(X_scaled, columns=feature_cols)

# 5) D√©finir X (features) et y (cible)
y = df['Income_class']  # Notre cible binaire (0: ‚â§50K, 1: >50K)

# 6) Split en train/test (80% / 20% comme sugg√©r√© dans l'√©nonc√©)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)  # stratify=y assure la m√™me proportion de classes dans train et test

print(f"\nTaille de l'ensemble d'apprentissage : {X_train.shape[0]} instances")
print(f"Taille de l'ensemble de test : {X_test.shape[0]} instances")
print(f"Proportion de classe 1 (>50K) dans train : {y_train.mean():.2%}")
print(f"Proportion de classe 1 (>50K) dans test : {y_test.mean():.2%}")

# ============================================
# APPRENTISSAGE ET TEST AVEC KNN
# ============================================

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

print("\n" + "=" * 60)
print("MOD√àLE KNN (K-Nearest Neighbors)")
print("=" * 60)

# 1) Cr√©er un KNN avec k=5 par d√©faut
knn = KNeighborsClassifier(n_neighbors=5)  # 5 voisins par d√©faut

# Entra√Æner le mod√®le sur la base d'apprentissage
print("\nEntra√Ænement du mod√®le KNN...")
knn.fit(X_train, y_train)
print("‚úì Mod√®le entra√Æn√©")

# Pr√©dictions sur l'ensemble d'apprentissage et de test
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# 1) Scores en apprentissage et en test
train_score = accuracy_score(y_train, y_train_pred)
test_score = accuracy_score(y_test, y_test_pred)

print(f"\n1) Scores du mod√®le KNN (k=5) :")
print(f"   - Score en apprentissage : {train_score:.4f} ({train_score*100:.2f}%)")
print(f"   - Score en test : {test_score:.4f} ({test_score*100:.2f}%)")

# Interpr√©tation : un √©cart de plus de 5-10% indiquerait du sur-apprentissage
if train_score - test_score > 0.1:
    print("   ‚ö† Attention : √©cart important indiquant possible sur-apprentissage")
else:
    print("   ‚úì √âcart raisonnable entre apprentissage et test")

# 2) Matrice de confusion
print("\n2) Matrice de confusion :")
cm = confusion_matrix(y_test, y_test_pred)
print("                  Pr√©dit")
print("                  N√©gatif  Positif")
print(f"R√©el N√©gatif     {cm[0,0]:6d}  {cm[0,1]:6d}")
print(f"R√©el Positif     {cm[1,0]:6d}  {cm[1,1]:6d}")

# Visualisation de la matrice de confusion
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['‚â§50K', '>50K'], 
            yticklabels=['‚â§50K', '>50K'])
plt.xlabel('Pr√©dit')
plt.ylabel('R√©el')
plt.title(f'Matrice de Confusion - KNN (k=5)\nAccuracy: {test_score:.2%}')
plt.tight_layout()
plt.savefig('confusion_matrix_knn.png', dpi=150)
plt.show()
print("‚úì Matrice de confusion sauvegard√©e")

# Calculer des m√©triques d√©taill√©es
print("\nRapport de classification d√©taill√© :")
print(classification_report(y_test, y_test_pred, 
                          target_names=['‚â§50K', '>50K']))

# Interpr√©tation de la matrice de confusion
print("\nObservations sur la matrice de confusion :")
tn, fp, fn, tp = cm.ravel()
print(f"   - Vrais N√©gatifs (correctement pr√©dits ‚â§50K) : {tn}")
print(f"   - Faux Positifs (pr√©dits >50K mais en r√©alit√© ‚â§50K) : {fp}")
print(f"   - Faux N√©gatifs (pr√©dits ‚â§50K mais en r√©alit√© >50K) : {fn}")
print(f"   - Vrais Positifs (correctement pr√©dits >50K) : {tp}")

# Calcul de m√©triques suppl√©mentaires
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"\nM√©triques pour la classe >50K :")
print(f"   - Pr√©cision : {precision:.4f} (quand on pr√©dit >50K, on a raison √† {precision:.2%})")
print(f"   - Rappel : {recall:.4f} (on d√©tecte {recall:.2%} des vrais >50K)")
print(f"   - F1-score : {f1:.4f} (moyenne harmonique des deux)")

# ============================================
# 2) √âTUDE DE L'INFLUENCE DU PARAM√àTRE k
# ============================================

print("\n" + "=" * 60)
print("√âTUDE DE L'INFLUENCE DU PARAM√àTRE k")
print("=" * 60)

# Tester diff√©rentes valeurs de k
k_values = range(1, 51)  # Tester k de 1 √† 50
train_scores = []
test_scores = []

print("\nCalcul des performances pour k = 1 √† 50...")
for k in k_values:
    knn_temp = KNeighborsClassifier(n_neighbors=k)
    knn_temp.fit(X_train, y_train)
    
    train_scores.append(knn_temp.score(X_train, y_train))
    test_scores.append(knn_temp.score(X_test, y_test))
    
    if k % 10 == 0:  # Afficher progression tous les 10 k
        print(f"  k={k:2d} : train={train_scores[-1]:.4f}, test={test_scores[-1]:.4f}")

# Visualisation de l'influence de k
plt.figure(figsize=(12, 6))

# Courbes d'√©volution
plt.subplot(1, 2, 1)
plt.plot(k_values, train_scores, 'b-', label='Score apprentissage', linewidth=2)
plt.plot(k_values, test_scores, 'r-', label='Score test', linewidth=2)
plt.xlabel('Valeur de k (nombre de voisins)')
plt.ylabel('Accuracy')
plt.title('Influence du param√®tre k sur les performances')
plt.legend()
plt.grid(True, alpha=0.3)

# Zoom sur les premi√®res valeurs
plt.subplot(1, 2, 2)
plt.plot(k_values[:20], train_scores[:20], 'b-', label='Train', linewidth=2)
plt.plot(k_values[:20], test_scores[:20], 'r-', label='Test', linewidth=2)
plt.xlabel('Valeur de k (nombre de voisins)')
plt.ylabel('Accuracy')
plt.title('Focus sur k=1 √† 20')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('knn_k_influence.png', dpi=150)
plt.show()

# Trouver le meilleur k
best_k_idx = np.argmax(test_scores)
best_k = k_values[best_k_idx]
best_score = test_scores[best_k_idx]

print(f"\nMeilleure performance en test :")
print(f"   - k optimal = {best_k}")
print(f"   - Accuracy = {best_score:.4f} ({best_score*100:.2f}%)")

# Analyse des observations
print("\nObservations sur l'influence de k :")
print("""
   - Quand k est petit (1-3) : 
        * Score train √©lev√© (souvent 1.0)
        * Score test variable, risque de sur-apprentissage
   - Quand k augmente (10-30) :
        * Score train diminue l√©g√®rement
        * Score test se stabilise, meilleure g√©n√©ralisation
   - Quand k est grand (>30) :
        * Les deux scores diminuent (sous-apprentissage)
   - Compromis optimal : k autour de {best_k} pour ce dataset
""".format(best_k=best_k))

# ============================================
# 3) REMPLACER KNN PAR D'AUTRES MOD√àLES
# ============================================

print("\n" + "=" * 60)
print("COMPARAISON AVEC D'AUTRES MOD√àLES")
print("=" * 60)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# Dictionnaire des mod√®les √† tester
models = {
    'KNN (optimal)': KNeighborsClassifier(n_neighbors=best_k),
    'R√©gression Logistique': LogisticRegression(max_iter=1000, random_state=42),
    'SVM (lin√©aire)': SVC(kernel='linear', random_state=42),
    'SVM (RBF)': SVC(kernel='rbf', random_state=42),
    'Arbre de d√©cision': DecisionTreeClassifier(random_state=42, max_depth=5)
}

results = []

print("\nEntra√Ænement et √©valuation des diff√©rents mod√®les...")
for name, model in models.items():
    # Entra√Ænement
    model.fit(X_train, y_train)
    
    # Pr√©dictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Scores
    train_acc = accuracy_score(y_train, y_pred_train)
    test_acc = accuracy_score(y_test, y_pred_test)
    
    results.append({
        'Mod√®le': name,
        'Train Accuracy': train_acc,
        'Test Accuracy': test_acc,
        '√âcart': train_acc - test_acc
    })
    
    print(f"\n{name} :")
    print(f"   Train: {train_acc:.4f} | Test: {test_acc:.4f} | √âcart: {train_acc-test_acc:.4f}")

# Cr√©er un DataFrame pour comparer
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Test Accuracy', ascending=False)

print("\n" + "=" * 60)
print("TABLEAU COMPARATIF DES PERFORMANCES")
print("=" * 60)
print(results_df.to_string(index=False))

# Visualisation comparative
plt.figure(figsize=(12, 6))
x = np.arange(len(results_df))
width = 0.35

plt.bar(x - width/2, results_df['Train Accuracy'], width, label='Train', alpha=0.8)
plt.bar(x + width/2, results_df['Test Accuracy'], width, label='Test', alpha=0.8)

plt.xlabel('Mod√®les')
plt.ylabel('Accuracy')
plt.title('Comparaison des performances des mod√®les')
plt.xticks(x, results_df['Mod√®le'], rotation=45, ha='right')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('model_comparison.png', dpi=150)
plt.show()

# ============================================
# 4) D√âDUIRE LE MOD√àLE OPTIMAL
# ============================================

print("\n" + "=" * 60)
print("S√âLECTION DU MOD√àLE OPTIMAL")
print("=" * 60)

# Trouver le meilleur mod√®le bas√© sur le score test
best_model_row = results_df.iloc[0]
best_model_name = best_model_row['Mod√®le']
best_model_test = best_model_row['Test Accuracy']

print(f"\nMeilleur mod√®le : {best_model_name}")
print(f"Accuracy en test : {best_model_test:.4f} ({best_model_test*100:.2f}%)")

# Crit√®res de s√©lection
print("\nCrit√®res pris en compte pour la s√©lection :")
print("""
   1. Performance en test (priorit√© principale)
   2. √âcart train-test (pour √©viter le sur-apprentissage)
   3. Complexit√© du mod√®le (plus simple = meilleure g√©n√©ralisation)
   4. Temps d'entra√Ænement (pour le d√©ploiement)
""")

# Analyser les forces/faiblesses de chaque mod√®le
print("\nAnalyse comparative :")
for idx, row in results_df.iterrows():
    print(f"\n{row['Mod√®le']} :")
    print(f"   - Test Accuracy: {row['Test Accuracy']:.2%}")
    print(f"   - √âcart train-test: {row['√âcart']:.2%}")
    if row['Mod√®le'] == best_model_name:
        print("   ‚≠ê MOD√àLE OPTIMAL")

# ============================================
# D√âPLOIEMENT DU MOD√àLE (sauvegarde)
# ============================================

import pickle

print("\n" + "=" * 60)
print("D√âPLOIEMENT DU MOD√àLE OPTIMAL")
print("=" * 60)

# Sauvegarder le meilleur mod√®le
best_model = models[best_model_name]  # R√©cup√©rer le mod√®le optimal

# Sauvegarde au format .pkl
with open('census.pkl', 'wb') as f:
    pickle.dump(best_model, f)

print(f"\n‚úì Mod√®le optimal sauvegard√© sous 'census.pkl'")
print(f"  - Mod√®le : {best_model_name}")
print(f"  - Accuracy test : {best_model_test:.2%}")

# Sauvegarder aussi le scaler pour pouvoir normaliser les futures donn√©es
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("‚úì Scaler sauvegard√© sous 'scaler.pkl'")

# D√©monstration de chargement du mod√®le
print("\nTest de chargement du mod√®le :")
with open('census.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

# V√©rifier que le mod√®le charg√© fonctionne
sample_pred = loaded_model.predict(X_test[:5])
print(f"  ‚úì Mod√®le charg√© avec succ√®s")
print(f"  ‚úì Pr√©dictions sur 5 √©chantillons : {sample_pred}")


# ============================================
# PARTIE 1-B : VALIDATION CROIS√âE
# ============================================

from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print("=" * 60)
print("PARTIE 1-B : VALIDATION CROIS√âE SUR ARBRE DE D√âCISION")
print("=" * 60)

# Recharger les donn√©es pr√©par√©es (on garde le m√™me split que pr√©c√©demment)
# X_train, X_test, y_train, y_test sont d√©j√† d√©finis

# ============================================
# 1. PR√âPARATION DES DONN√âES (d√©j√† fait)
# ============================================

print("\n1. Pr√©paration des donn√©es :")
print("   ‚úì Valeurs manquantes trait√©es")
print("   ‚úì Variables cat√©gorielles encod√©es")
print("   ‚úì Donn√©es normalis√©es (optionnel pour arbres)")
print("   ‚úì Split train/test (80/20) effectu√©")

# Note : Les arbres de d√©cision n'ont pas besoin de normalisation
# mais on garde les donn√©es normalis√©es pour la coh√©rence

# ============================================
# 2. CLASSIFIEUR CONSTANT (R√âF√âRENTIEL √Ä BATTRE)
# ============================================

print("\n" + "=" * 60)
print("2. CLASSIFIEUR CONSTANT (R√âF√âRENTIEL)")
print("=" * 60)

# Le classifieur constant pr√©dit toujours la classe majoritaire
from sklearn.dummy import DummyClassifier

# Cr√©er un classifieur constant qui pr√©dit la classe la plus fr√©quente
dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)

# Pr√©dictions et erreur
y_pred_dummy = dummy.predict(X_test)
dummy_accuracy = accuracy_score(y_test, y_pred_dummy)
dummy_error = 1 - dummy_accuracy

print(f"\nClasse majoritaire dans l'ensemble d'apprentissage :")
print(f"   - Classe 0 (‚â§50K) : {(y_train == 0).sum()} instances ({y_train.mean()*100:.1f}%)")
print(f"   - Classe 1 (>50K) : {(y_train == 1).sum()} instances ({(1-y_train.mean())*100:.1f}%)")

print(f"\nPerformance du classifieur constant :")
print(f"   - Accuracy : {dummy_accuracy:.4f} ({dummy_accuracy*100:.2f}%)")
print(f"   - Erreur de test : {dummy_error:.4f} ({dummy_error*100:.2f}%)")

print(f"\nüìå Ce score de {dummy_accuracy:.2%} est notre r√©f√©rentiel √† battre !")
print(f"   Tout mod√®le performant doit faire mieux que √ßa.")

# ============================================
# 3. CONSTRUCTION D'UN PREMIER ARBRE DE D√âCISION
# ============================================

print("\n" + "=" * 60)
print("3. PREMIER ARBRE DE D√âCISION (peu profond)")
print("=" * 60)

# Cr√©er un arbre volontairement petit pour visualisation
tree_small = DecisionTreeClassifier(
    max_depth=3,           # Arbre peu profond (3 niveaux)
    min_samples_split=20,  # Minimum d'√©chantillons pour diviser un noeud
    min_samples_leaf=10,   # Minimum d'√©chantillons par feuille
    random_state=42
)

# Entra√Ænement
tree_small.fit(X_train, y_train)

# √âvaluation
y_train_pred_small = tree_small.predict(X_train)
y_test_pred_small = tree_small.predict(X_test)

train_acc_small = accuracy_score(y_train, y_train_pred_small)
test_acc_small = accuracy_score(y_test, y_test_pred_small)

print(f"\nPerformance de l'arbre (max_depth=3) :")
print(f"   - Accuracy train : {train_acc_small:.4f} ({train_acc_small*100:.2f}%)")
print(f"   - Accuracy test  : {test_acc_small:.4f} ({test_acc_small*100:.2f}%)")
print(f"   - Erreur test    : {1-test_acc_small:.4f} ({(1-test_acc_small)*100:.2f}%)")

# Comparaison avec le classifieur constant
improvement = (test_acc_small - dummy_accuracy) / dummy_accuracy * 100
print(f"\nComparaison avec le classifieur constant :")
print(f"   - Am√©lioration : +{improvement:.1f}%")

# ============================================
# 3b. VISUALISATION DE L'ARBRE (avec graphviz)
# ============================================

print("\n" + "-" * 40)
print("Visualisation de l'arbre")

# M√©thode 1 : avec matplotlib (simplifi√©)
plt.figure(figsize=(20, 10))
plot_tree(tree_small, 
          feature_names=X_train.columns.tolist(),
          class_names=['‚â§50K', '>50K'],
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)  # Limiter l'affichage √† 3 niveaux
plt.title("Arbre de d√©cision (max_depth=3)")
plt.tight_layout()
plt.savefig('decision_tree_small.png', dpi=150, bbox_inches='tight')
plt.show()
print("‚úì Arbre sauvegard√© sous 'decision_tree_small.png'")

# M√©thode 2 : exporter au format .dot pour graphviz (optionnel)
from sklearn.tree import export_graphviz
export_graphviz(tree_small, 
                out_file='tree.dot',
                feature_names=X_train.columns.tolist(),
                class_names=['‚â§50K', '>50K'],
                filled=True, rounded=True,
                special_characters=True)
print("‚úì Fichier .dot cr√©√© (peut √™tre visualis√© avec Graphviz)")

# Interpr√©tation des features importantes
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': tree_small.feature_importances_
}).sort_values('importance', ascending=False)

print("\nFeatures les plus importantes (top 5) :")
print(feature_importance.head(10).to_string(index=False))

# ============================================
# 4. √âTUDE DE L'INFLUENCE DE max_depth
# ============================================

print("\n" + "=" * 60)
print("4. INFLUENCE DU PARAM√àTRE max_depth")
print("=" * 60)

# Tester diff√©rentes profondeurs
depths = range(1, 21)  # de 1 √† 20
train_scores_depth = []
test_scores_depth = []
tree_models = []

print("Entra√Ænement des arbres avec diff√©rentes profondeurs...")
for depth in depths:
    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)
    tree.fit(X_train, y_train)
    
    train_scores_depth.append(tree.score(X_train, y_train))
    test_scores_depth.append(tree.score(X_test, y_test))
    tree_models.append(tree)
    
    if depth % 5 == 0:
        print(f"  depth={depth:2d} : train={train_scores_depth[-1]:.4f}, test={test_scores_depth[-1]:.4f}")

# Visualisation
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(depths, train_scores_depth, 'b-', label='Train', linewidth=2)
plt.plot(depths, test_scores_depth, 'r-', label='Test', linewidth=2)
plt.axhline(y=dummy_accuracy, color='g', linestyle='--', label='Classifieur constant')
plt.xlabel('Profondeur maximale (max_depth)')
plt.ylabel('Accuracy')
plt.title('Influence de la profondeur sur les performances')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Calculer l'√©cart train-test (indicateur de sur-apprentissage)
gap = np.array(train_scores_depth) - np.array(test_scores_depth)
plt.plot(depths, gap, 'purple', linewidth=2)
plt.xlabel('Profondeur maximale (max_depth)')
plt.ylabel('√âcart Train - Test')
plt.title("√âcart d'accuracy (indicateur de sur-apprentissage)")
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('tree_depth_influence.png', dpi=150)
plt.show()

# Trouver la meilleure profondeur
best_depth_idx = np.argmax(test_scores_depth)
best_depth = depths[best_depth_idx]
best_depth_score = test_scores_depth[best_depth_idx]

print(f"\nMeilleure profondeur (sur test) : depth={best_depth}")
print(f"   - Accuracy test : {best_depth_score:.4f} ({best_depth_score*100:.2f}%)")
print(f"   - Accuracy train : {train_scores_depth[best_depth_idx]:.4f}")
print(f"   - √âcart : {train_scores_depth[best_depth_idx] - best_depth_score:.4f}")

# ============================================
# 5. VALIDATION CROIS√âE AVEC GridSearchCV
# ============================================

print("\n" + "=" * 60)
print("5. VALIDATION CROIS√âE AVEC GridSearchCV")
print("=" * 60)

# D√©finir la grille de param√®tres √† tester
param_grid = {
    'max_depth': [3, 5, 7, 10, 15, 20, None],  # None = pas de limite
    'min_samples_split': [2, 5, 10, 20, 50],
    'min_samples_leaf': [1, 2, 5, 10, 20],
    'criterion': ['gini', 'entropy']
}

print(f"Grille de param√®tres √† tester :")
print(f"   - max_depth : {param_grid['max_depth']}")
print(f"   - min_samples_split : {param_grid['min_samples_split']}")
print(f"   - min_samples_leaf : {param_grid['min_samples_leaf']}")
print(f"   - criterion : {param_grid['criterion']}")
print(f"\nNombre total de combinaisons : {np.prod([len(v) for v in param_grid.values()])}")

# Cr√©er le GridSearchCV avec validation crois√©e (5 folds)
print("\nLancement du GridSearchCV (peut prendre quelques minutes)...")
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,                    # Validation crois√©e 5-fold
    scoring='accuracy',      # M√©trique d'√©valuation
    n_jobs=-1,               # Utiliser tous les processeurs
    verbose=1                # Afficher la progression
)

# Entra√Æner le grid search
grid_search.fit(X_train, y_train)

# R√©sultats
print("\n" + "=" * 40)
print("R√âSULTATS DU GRIDSEARCH")
print("=" * 40)

print(f"\nMeilleurs param√®tres trouv√©s :")
for param, value in grid_search.best_params_.items():
    print(f"   - {param} : {value}")

print(f"\nMeilleur score de validation crois√©e : {grid_search.best_score_:.4f} ({grid_search.best_score_*100:.2f}%)")

# √âvaluer sur l'ensemble de test
best_tree = grid_search.best_estimator_
y_test_pred_best = best_tree.predict(X_test)
test_score_best = accuracy_score(y_test, y_test_pred_best)

print(f"\nPerformance sur l'ensemble de test :")
print(f"   - Accuracy : {test_score_best:.4f} ({test_score_best*100:.2f}%)")

# Comparaison avec la profondeur optimale simple
print(f"\nComparaison avec la s√©lection simple (max_depth uniquement) :")
print(f"   - S√©lection simple (max_depth={best_depth}) : {best_depth_score:.2%}")
print(f"   - GridSearch (param√®tres optimis√©s) : {test_score_best:.2%}")
improvement_grid = (test_score_best - best_depth_score) / best_depth_score * 100
print(f"   - Am√©lioration : +{improvement_grid:.1f}%")

# ============================================
# 5b. VISUALISATION DE L'ERREUR DE VALIDATION CROIS√âE
# ============================================

print("\n" + "-" * 40)
print("Visualisation des r√©sultats du GridSearch")

# Extraire les r√©sultats du grid search
cv_results = pd.DataFrame(grid_search.cv_results_)

# Visualiser l'influence de max_depth en gardant les autres params fixes
# Filtrer pour min_samples_split=5, min_samples_leaf=1, criterion='gini' (par exemple)
filtered_results = cv_results[
    (cv_results['param_min_samples_split'] == 5) &
    (cv_results['param_min_samples_leaf'] == 1) &
    (cv_results['param_criterion'] == 'gini')
].copy()

# Remplacer None par une grande valeur pour l'affichage
filtered_results['param_max_depth_display'] = filtered_results['param_max_depth'].apply(
    lambda x: 30 if x is None else x
)
filtered_results = filtered_results.sort_values('param_max_depth_display')

plt.figure(figsize=(10, 6))
plt.plot(filtered_results['param_max_depth_display'], 
         filtered_results['mean_test_score'], 
         'b-o', label='Score validation crois√©e', linewidth=2)
plt.fill_between(filtered_results['param_max_depth_display'],
                  filtered_results['mean_test_score'] - filtered_results['std_test_score'],
                  filtered_results['mean_test_score'] + filtered_results['std_test_score'],
                  alpha=0.2, color='b')

plt.xlabel('max_depth')
plt.ylabel('Accuracy moyenne (validation crois√©e)')
plt.title("Erreur de validation crois√©e en fonction de max_depth\n(autres param√®tres fix√©s)")
plt.grid(True, alpha=0.3)
plt.legend()

# Marquer le meilleur point
best_point = filtered_results.loc[filtered_results['mean_test_score'].idxmax()]
plt.plot(best_point['param_max_depth_display'], 
         best_point['mean_test_score'], 
         'r*', markersize=15, label='Meilleur point')

plt.tight_layout()
plt.savefig('cv_results_depth.png', dpi=150)
plt.show()

# ============================================
# 6. √âVALUATION FINALE DU CLASSIFIEUR OPTIMIS√â
# ============================================

print("\n" + "=" * 60)
print("6. √âVALUATION FINALE - ARBRE OPTIMIS√â")
print("=" * 60)

# Matrice de confusion pour l'arbre optimis√©
cm_best = confusion_matrix(y_test, y_test_pred_best)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Greens',
            xticklabels=['‚â§50K', '>50K'],
            yticklabels=['‚â§50K', '>50K'])
plt.xlabel('Pr√©dit')
plt.ylabel('R√©el')
plt.title(f"Matrice de confusion - Arbre optimis√©\nAccuracy: {test_score_best:.2%}")
plt.tight_layout()
plt.savefig('confusion_matrix_best_tree.png', dpi=150)
plt.show()

# Rapport de classification d√©taill√©
from sklearn.metrics import classification_report
print("\nRapport de classification - Arbre optimis√© :")
print(classification_report(y_test, y_test_pred_best, 
                          target_names=['‚â§50K', '>50K']))

# Comparaison finale avec tous les mod√®les
print("\n" + "=" * 40)
print("COMPARAISON FINALE")
print("=" * 40)

comparison = pd.DataFrame({
    'Mod√®le': ['Classifieur constant', 'Arbre simple (depth=3)', 
               'Arbre optimis√© (GridSearch)'],
    'Accuracy test': [dummy_accuracy, test_acc_small, test_score_best],
    'Am√©lioration vs constant': ['-', 
        f"{(test_acc_small-dummy_accuracy)/dummy_accuracy*100:.1f}%",
        f"{(test_score_best-dummy_accuracy)/dummy_accuracy*100:.1f}%"]
})

print(comparison.to_string(index=False))

# ============================================
# CONCLUSION DE LA PARTIE 1-B
# ============================================

print("\n" + "=" * 60)
print("CONCLUSIONS DE LA PARTIE 1-B")
print("=" * 60)

print("""
Points cl√©s √† retenir :

1. Classifieur constant : 
   - Baseline √† battre : {dummy:.2%} accuracy
   - Important pour mesurer l'apport r√©el des mod√®les

2. Influence de max_depth :
   - Profondeur trop faible ‚Üí sous-apprentissage
   - Profondeur trop grande ‚Üí sur-apprentissage
   - Optimal trouv√© √† depth={best_depth}

3. Validation crois√©e :
   - Permet d'√©viter le sur-apprentissage
   - GridSearch explore automatiquement les combinaisons
   - Meilleurs param√®tres : {best_params}

4. Am√©lioration obtenue :
   - {improve_vs_dummy:.1f}% par rapport au classifieur constant
   - {improve_vs_simple:.1f}% par rapport √† l'arbre simple

5. Prochaine √©tape :
   - Appliquer ces techniques aux m√©thodes d'ensemble (Random Forest, Boosting)
""".format(
    dummy=dummy_accuracy,
    best_depth=best_depth,
    best_params=grid_search.best_params_,
    improve_vs_dummy=(test_score_best/dummy_accuracy-1)*100,
    improve_vs_simple=(test_score_best/test_acc_small-1)*100
))

# Sauvegarder le meilleur arbre pour utilisation future
with open('best_tree.pkl', 'wb') as f:
    pickle.dump(best_tree, f)
print("\n‚úì Meilleur arbre sauvegard√© sous 'best_tree.pkl'")


# ============================================
# PARTIE 1-C : BAGGING ET RANDOM FOREST
# ============================================

from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import time
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print("=" * 60)
print("PARTIE 1-C : BAGGING ET RANDOM FOREST")
print("=" * 60)

# ============================================
# 1. BAGGING AVEC RandomForestClassifier
# ============================================

print("\n" + "=" * 60)
print("1. BAGGING - PRINCIPE ET IMPL√âMENTATION")
print("=" * 60)

print("""
Le Bagging (Bootstrap Aggregating) :
- Cr√©e B √©chantillons bootstrap (tirage al√©atoire avec remise)
- Entra√Æne un arbre sur chaque √©chantillon
- Agr√®ge les pr√©dictions par vote majoritaire
- R√©duit la variance sans augmenter le biais
""")

# Comment utiliser RandomForestClassifier pour faire du Bagging ?
# Pour faire du Bagging "pur" (sans s√©lection al√©atoire des features),
# il faut fixer max_features = n_features (prendre toutes les features)

n_features = X_train.shape[1]
print(f"\nNombre total de features : {n_features}")

# Bagging avec des arbres (max_features = n_features)
bagging_rf = RandomForestClassifier(
    n_estimators=100,           # B = 100 arbres
    max_features=n_features,    # Prendre toutes les features = Bagging pur
    bootstrap=True,              # √âchantillonnage bootstrap
    oob_score=True,              # Calculer le score Out-Of-Bag
    random_state=42,
    n_jobs=-1                    # Utiliser tous les processeurs
)

print("\nEntra√Ænement du Bagging (100 arbres, toutes les features)...")
start_time = time.time()
bagging_rf.fit(X_train, y_train)
bagging_time = time.time() - start_time

print(f"‚úì Entra√Ænement termin√© en {bagging_time:.2f} secondes")

# √âvaluation
y_train_pred_bag = bagging_rf.predict(X_train)
y_test_pred_bag = bagging_rf.predict(X_test)

train_acc_bag = accuracy_score(y_train, y_train_pred_bag)
test_acc_bag = accuracy_score(y_test, y_test_pred_bag)

print(f"\nPerformances du Bagging :")
print(f"   - Accuracy train : {train_acc_bag:.4f} ({train_acc_bag*100:.2f}%)")
print(f"   - Accuracy test  : {test_acc_bag:.4f} ({test_acc_bag*100:.2f}%)")
print(f"   - Score OOB (Out-of-Bag) : {bagging_rf.oob_score_:.4f} ({bagging_rf.oob_score_*100:.2f}%)")

# Comparaison avec l'arbre simple
print(f"\nComparaison avec l'arbre optimis√© :")
print(f"   - Arbre optimis√© : {test_score_best:.2%}")
print(f"   - Bagging : {test_acc_bag:.2%}")
improvement = (test_acc_bag - test_score_best) / test_score_best * 100
print(f"   - Am√©lioration : +{improvement:.1f}%")

# ============================================
# 1b. √âTUDE DE LA COMPLEXIT√â ET PERFORMANCE SELON B
# ============================================

print("\n" + "-" * 40)
print("Influence du nombre d'arbres (B) sur les performances")

# Tester diff√©rentes valeurs de B (n_estimators)
B_values = [1, 5, 10, 20, 50, 100, 200, 300]
train_scores_b = []
test_scores_b = []
oob_scores_b = []
times_b = []

print("\nEntra√Ænement pour diff√©rentes valeurs de B...")
for B in B_values:
    start = time.time()
    rf_temp = RandomForestClassifier(
        n_estimators=B,
        max_features=n_features,
        bootstrap=True,
        oob_score=True,
        random_state=42,
        n_jobs=-1
    )
    rf_temp.fit(X_train, y_train)
    
    train_scores_b.append(rf_temp.score(X_train, y_train))
    test_scores_b.append(rf_temp.score(X_test, y_test))
    oob_scores_b.append(rf_temp.oob_score_)
    times_b.append(time.time() - start)
    
    print(f"  B={B:3d} : train={train_scores_b[-1]:.4f}, test={test_scores_b[-1]:.4f}, "
          f"oob={oob_scores_b[-1]:.4f}, temps={times_b[-1]:.2f}s")

# Visualisation
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Graphique 1 : √âvolution des scores
axes[0].plot(B_values, train_scores_b, 'b-o', label='Train', linewidth=2)
axes[0].plot(B_values, test_scores_b, 'r-o', label='Test', linewidth=2)
axes[0].plot(B_values, oob_scores_b, 'g-o', label='OOB', linewidth=2)
axes[0].axhline(y=test_score_best, color='orange', linestyle='--', 
                label='Arbre optimis√©', alpha=0.7)
axes[0].set_xlabel('Nombre d\'arbres (B)')
axes[0].set_ylabel('Accuracy')
axes[0].set_title('Performance vs Nombre d\'arbres')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Graphique 2 : √âcart train-test (sur-apprentissage)
gap_b = np.array(train_scores_b) - np.array(test_scores_b)
axes[1].plot(B_values, gap_b, color="purple", marker="o", linewidth=2)
axes[1].set_xlabel('Nombre d\'arbres (B)')
axes[1].set_ylabel('√âcart Train - Test')
axes[1].set_title('√âcart (indicateur de sur-apprentissage)')
axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
axes[1].grid(True, alpha=0.3)

# Graphique 3 : Temps d'entra√Ænement
axes[2].plot(B_values, times_b, 'b-o', linewidth=2)
axes[2].set_xlabel('Nombre d\'arbres (B)')
axes[2].set_ylabel('Temps (secondes)')
axes[2].set_title('Complexit√© temporelle')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('bagging_B_analysis.png', dpi=150)
plt.show()

print("\nObservations sur la complexit√© et performance :")
print("""
   - Quand B augmente (1 ‚Üí 50) :
        * Performance s'am√©liore rapidement
        * Temps d'entra√Ænement augmente lin√©airement
   - Quand B > 100 :
        * Gains de performance marginaux (loi des rendements d√©croissants)
        * Le score se stabilise autour de {:.2%}
   - Compromis optimal : B ‚âà 100-200 (bonne performance / temps raisonnable)
   - Le score OOB est un bon estimateur de l'erreur de test (√©conomise la validation crois√©e)
""".format(test_scores_b[-1]))


# ============================================
# 2. RANDOM FOREST (AVEC S√âLECTION AL√âATOIRE DES FEATURES)
# ============================================

print("\n" + "=" * 60)
print("2. RANDOM FORET (AVEC S√âLECTION AL√âATOIRE DES FEATURES)")
print("=" * 60)

print("""
Diff√©rence avec le Bagging :
- Bagging : toutes les features sont consid√©r√©es √† chaque split
- Random Forest : on s√©lectionne al√©atoirement p features √† chaque split
- p est g√©n√©ralement sqrt(n_features) pour la classification
""")

# Choisir une valeur pour p (param√®tre max_features)
p_sqrt = int(np.sqrt(n_features))
p_log = int(np.log2(n_features)) + 1
p_half = n_features // 2

print(f"\nValeurs possibles pour p (max_features) :")
print(f"   - sqrt(n_features) = {p_sqrt}")
print(f"   - log2(n_features) = {p_log}")
print(f"   - n_features/2 = {p_half}")
print(f"   - Toutes les features (Bagging) = {n_features}")

# Construire une Random Forest avec p = sqrt(n_features)
rf_sqrt = RandomForestClassifier(
    n_estimators=100,
    max_features=p_sqrt,      # sqrt(n_features)
    bootstrap=True,
    oob_score=True,
    random_state=42,
    n_jobs=-1
)

print(f"\nEntra√Ænement de la Random Forest (p={p_sqrt})...")
rf_sqrt.fit(X_train, y_train)

# √âvaluation
y_test_pred_rf = rf_sqrt.predict(X_test)
test_acc_rf = accuracy_score(y_test, y_test_pred_rf)
oob_score_rf = rf_sqrt.oob_score_

print(f"\nPerformances de la Random Forest (p={p_sqrt}) :")
print(f"   - Accuracy test : {test_acc_rf:.4f} ({test_acc_rf*100:.2f}%)")
print(f"   - Score OOB : {oob_score_rf:.4f} ({oob_score_rf*100:.2f}%)")

# Comparaison Bagging vs Random Forest
print(f"\nComparaison Bagging vs Random Forest :")
print(f"   - Bagging (p={n_features}) : {test_acc_bag:.2%}")
print(f"   - Random Forest (p={p_sqrt}) : {test_acc_rf:.2%}")
improvement_rf = (test_acc_rf - test_acc_bag) / test_acc_bag * 100
print(f"   - Diff√©rence : {improvement_rf:+.1f}%")

# ============================================
# 3. ERREUR OUT-OF-BAG (OOB)
# ============================================

print("\n" + "=" * 60)
print("3. ERREUR OUT-OF-BAG (OOB)")
print("=" * 60)

print("""
Principe de l'erreur OOB :
- Pour chaque arbre, environ 1/3 des donn√©es ne sont pas utilis√©es (out-of-bag)
- Ces donn√©es servent de validation naturelle
- La moyenne des erreurs OOB sur tous les arbres est un estimateur non biais√©
- Utile quand on a pas de validation crois√©e
""")

print(f"\nPour la Random Forest (p={p_sqrt}) :")
print(f"   - Erreur OOB : {1-oob_score_rf:.4f} ({(1-oob_score_rf)*100:.2f}%)")
print(f"   - Erreur test : {1-test_acc_rf:.4f} ({(1-test_acc_rf)*100:.2f}%)")
print(f"   - √âcart OOB vs test : {abs(oob_score_rf - test_acc_rf):.4f}")

# V√©rification sur diff√©rentes valeurs de B
B_values_oob = [10, 50, 100, 200]
oob_errors = []
test_errors = []

print("\n√âvolution de l'erreur OOB avec B :")
for B in B_values_oob:
    rf_temp = RandomForestClassifier(
        n_estimators=B,
        max_features=p_sqrt,
        bootstrap=True,
        oob_score=True,
        random_state=42,
        n_jobs=-1
    )
    rf_temp.fit(X_train, y_train)
    
    oob_errors.append(1 - rf_temp.oob_score_)
    test_errors.append(1 - rf_temp.score(X_test, y_test))
    print(f"  B={B:3d} : OOB error={oob_errors[-1]:.4f}, Test error={test_errors[-1]:.4f}")

# Visualisation
plt.figure(figsize=(8, 5))
plt.plot(B_values_oob, oob_errors, 'b-o', label='Erreur OOB', linewidth=2)
plt.plot(B_values_oob, test_errors, 'r-o', label='Erreur Test', linewidth=2)
plt.xlabel('Nombre d\'arbres (B)')
plt.ylabel("Taux d'erreur")
plt.title('Comparaison erreur OOB vs erreur Test')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('oob_vs_test.png', dpi=150)
plt.show()

# ============================================
# 4. VALIDATION CROIS√âE SUR LE PARAM√àTRE p
# ============================================

print("\n" + "=" * 60)
print("4. VALIDATION CROIS√âE SUR LE PARAM√àTRE p (max_features)")
print("=" * 60)

# Fixer B √† une valeur raisonnable (100)
B_fixed = 100

# Tester diff√©rentes valeurs de p
p_values = [1, 2, 3, 5, 10, 20, 30, n_features//4, n_features//2, n_features]
p_values = [p for p in p_values if p <= n_features]  # Garder les valeurs valides
p_values = sorted(set(p_values))  # Enlever les doublons

print(f"\nTest des valeurs de p avec B={B_fixed} arbres :")
print(f"Valeurs test√©es : {p_values}")

cv_scores_p = []
test_scores_p = []
oob_scores_p = []

for p in p_values:
    rf_temp = RandomForestClassifier(
        n_estimators=B_fixed,
        max_features=p,
        bootstrap=True,
        oob_score=True,
        random_state=42,
        n_jobs=-1
    )
    
    # Validation crois√©e (5-fold) pour √™tre plus robuste
    cv_score = np.mean(cross_val_score(rf_temp, X_train, y_train, cv=5))
    
    # Entra√Ænement complet pour le score test et OOB
    rf_temp.fit(X_train, y_train)
    test_score = rf_temp.score(X_test, y_test)
    
    cv_scores_p.append(cv_score)
    test_scores_p.append(test_score)
    oob_scores_p.append(rf_temp.oob_score_)
    
    print(f"  p={p:3d} : CV={cv_score:.4f}, Test={test_score:.4f}, OOB={rf_temp.oob_score_:.4f}")

# Visualisation
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(p_values, cv_scores_p, 'b-o', label='Validation crois√©e', linewidth=2)
plt.plot(p_values, test_scores_p, 'r-o', label='Test', linewidth=2)
plt.plot(p_values, oob_scores_p, 'g-o', label='OOB', linewidth=2)
plt.xlabel('p (max_features)')
plt.ylabel('Accuracy')
plt.title(f'Influence de p (B={B_fixed} arbres)')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# Zoom sur les petites valeurs de p
p_small = [p for p in p_values if p <= 20]
idx_small = [i for i, p in enumerate(p_values) if p <= 20]
plt.plot(p_small, [cv_scores_p[i] for i in idx_small], 'b-o', label='CV')
plt.plot(p_small, [test_scores_p[i] for i in idx_small], 'r-o', label='Test')
plt.xlabel('p (max_features) - zoom')
plt.ylabel('Accuracy')
plt.title('Focus sur p ‚â§ 20')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('rf_p_optimization.png', dpi=150)
plt.show()

# Trouver le meilleur p
best_p_idx = np.argmax(test_scores_p)
best_p = p_values[best_p_idx]
best_p_score = test_scores_p[best_p_idx]

print(f"\nMeilleure valeur de p : {best_p}")
print(f"   - Score test correspondant : {best_p_score:.4f} ({best_p_score*100:.2f}%)")
print(f"   - Score CV : {cv_scores_p[best_p_idx]:.4f}")
print(f"   - Score OOB : {oob_scores_p[best_p_idx]:.4f}")

# ============================================
# 5. RANDOM FOREST OPTIMIS√âE
# ============================================

print("\n" + "=" * 60)
print("5. RANDOM FOREST OPTIMIS√âE")
print("=" * 60)

# Construire la Random Forest avec les meilleurs param√®tres
rf_optimized = RandomForestClassifier(
    n_estimators=B_fixed,
    max_features=best_p,
    bootstrap=True,
    oob_score=True,
    random_state=42,
    n_jobs=-1
)

rf_optimized.fit(X_train, y_train)

# √âvaluation finale
y_test_pred_rf_opt = rf_optimized.predict(X_test)
test_acc_rf_opt = accuracy_score(y_test, y_test_pred_rf_opt)

print(f"\nPerformances de la Random Forest optimis√©e :")
print(f"   - Param√®tres : B={B_fixed}, p={best_p}")
print(f"   - Accuracy test : {test_acc_rf_opt:.4f} ({test_acc_rf_opt*100:.2f}%)")
print(f"   - Score OOB : {rf_optimized.oob_score_:.4f}")

# Matrice de confusion
cm_rf = confusion_matrix(y_test, y_test_pred_rf_opt)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Oranges',
            xticklabels=['‚â§50K', '>50K'],
            yticklabels=['‚â§50K', '>50K'])
plt.xlabel('Pr√©dit')
plt.ylabel('R√©el')
plt.title(f'Matrice de confusion - Random Forest optimis√©e\nAccuracy: {test_acc_rf_opt:.2%}')
plt.tight_layout()
plt.savefig('confusion_matrix_rf.png', dpi=150)
plt.show()

# ============================================
# 6. COMPARAISON FINALE
# ============================================

print("\n" + "=" * 60)
print("6. COMPARAISON FINALE DES MOD√àLES")
print("=" * 60)

comparison_final = pd.DataFrame({
    'Mod√®le': [
        'Classifieur constant',
        'Arbre optimis√©',
        'Bagging (B=100)',
        'Random Forest (p=sqrt)',
        'Random Forest optimis√©e'
    ],
    'Accuracy test': [
        dummy_accuracy,
        test_score_best,
        test_acc_bag,
        test_acc_rf,
        test_acc_rf_opt
    ]
})

print("\nTableau comparatif :")
print(comparison_final.to_string(index=False))

# Visualisation comparative
plt.figure(figsize=(10, 6))
bars = plt.bar(comparison_final['Mod√®le'], comparison_final['Accuracy test'], 
               color=['gray', 'lightblue', 'blue', 'orange', 'red'])
plt.xlabel('Mod√®les')
plt.ylabel('Accuracy')
plt.title('Comparaison des performances des mod√®les')
plt.xticks(rotation=45, ha='right')

# Ajouter les valeurs sur les barres
for bar, val in zip(bars, comparison_final['Accuracy test']):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{val:.2%}', ha='center', va='bottom')

plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('final_model_comparison.png', dpi=150)
plt.show()

print("\n" + "=" * 60)
print("CONCLUSIONS DE LA PARTIE 1-C")
print("=" * 60)

print("""
Points cl√©s √† retenir :

1. Bagging :
   - R√©duit la variance par rapport √† un seul arbre
   - Performance augmente avec B jusqu'√† saturation
   - Score OOB = bon estimateur de l'erreur de test

2. Random Forest :
   - Am√©liore encore le Bagging en d√©corr√©lant les arbres
   - Le param√®tre p (max_features) est crucial
   - p optimal ‚âà sqrt(n_features) pour la classification

3. Gains obtenus :
   - Arbre optimis√© ‚Üí Bagging : +{gain_bag:.1f}%
   - Bagging ‚Üí Random Forest : +{gain_rf:.1f}%
   - Gain total vs classifieur constant : +{gain_total:.1f}%

4. Prochaine √©tape :
   - Exp√©rimenter le Boosting (Gradient Boosting)
""".format(
    gain_bag=(test_acc_bag/test_score_best-1)*100,
    gain_rf=(test_acc_rf_opt/test_acc_bag-1)*100,
    gain_total=(test_acc_rf_opt/dummy_accuracy-1)*100
))

# Sauvegarder la Random Forest optimis√©e
with open('random_forest_optimized.pkl', 'wb') as f:
    pickle.dump(rf_optimized, f)
print("\n‚úì Random Forest optimis√©e sauvegard√©e sous 'random_forest_optimized.pkl'")


# ============================================
# PARTIE 1-D : BOOSTING (GRADIENT BOOSTING)
# ============================================

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time

print("=" * 60)
print("PARTIE 1-D : BOOSTING AVEC GRADIENT BOOSTING")
print("=" * 60)

print("""
Le Boosting (AdaBoost / Gradient Boosting) :
- Construit les arbres s√©quentiellement
- Chaque nouvel arbre corrige les erreurs du pr√©c√©dent
- Les arbres sont g√©n√©ralement peu profonds (stumps)
- Tr√®s puissant mais risque de sur-apprentissage
""")

# V√©rifier la version de scikit-learn
import sklearn
print(f"\nVersion de scikit-learn : {sklearn.__version__}")
if sklearn.__version__ >= '0.20':
    print("‚úì Version suffisante pour les fonctionnalit√©s avanc√©es")
else:
    print("‚ö† Version ancienne, certaines fonctionnalit√©s peuvent manquer")

# ============================================
# a) IDENTIFICATION DES PARAM√àTRES CRUCIAUX
# ============================================

print("\n" + "=" * 60)
print("a) PARAM√àTRES CRUCIAUX DE GRADIENT BOOSTING")
print("=" * 60)

print("""
Param√®tres les plus importants :

1. n_estimators (B) : nombre d'arbres dans la s√©quence
   - Trop peu ‚Üí sous-apprentissage
   - Trop ‚Üí sur-apprentissage (d'o√π l'int√©r√™t de l'early stopping)

2. learning_rate (ŒΩ - nu) : taux d'apprentissage
   - Poids accord√© √† chaque nouvel arbre
   - G√©n√©ralement petit (0.01 √† 0.1)
   - Compromis avec n_estimators : plus learning_rate est petit,
     plus il faut d'arbres

3. max_depth : profondeur des arbres
   - Pour le boosting, on utilise souvent des arbres peu profonds (3-5)
   - Des arbres trop profonds ‚Üí sur-apprentissage rapide

4. subsample : fraction des donn√©es utilis√©e pour chaque arbre
   - Introduit de l'al√©atoire (comme Random Forest)
   - Valeur typique : 0.5 √† 1.0

5. min_samples_split / min_samples_leaf : pour contr√¥ler la complexit√©

6. validation_fraction / n_iter_no_change : pour l'early stopping
   - validation_fraction : proportion pour la validation
   - n_iter_no_change : nombre d'it√©rations sans am√©lioration pour arr√™ter
""")

print("\nParam√®tres correspondant √† AdaBoost dans GradientBoosting :")
print("""
AdaBoost peut √™tre simul√© avec GradientBoosting en utilisant :
   - loss='exponential' (au lieu de 'deviance' par d√©faut)
   - learning_rate plus √©lev√©
   - max_depth=1 (stumps)
   
Mais la vraie impl√©mentation d'AdaBoost est dans sklearn.ensemble.AdaBoostClassifier
""")

# ============================================
# b) S√âLECTION DE B AVEC EARLY STOPPING
# ============================================

print("\n" + "=" * 60)
print("b) S√âLECTION DE B AVEC EARLY STOPPING")
print("=" * 60)

# Fixer learning_rate et max_depth (on les optimisera plus tard)
learning_rate_fixed = 0.1
max_depth_fixed = 3

print(f"\nParam√®tres fix√©s :")
print(f"   - learning_rate = {learning_rate_fixed}")
print(f"   - max_depth = {max_depth_fixed}")

# Cr√©er un Gradient Boosting avec early stopping
gb_early = GradientBoostingClassifier(
    n_estimators=1000,              # Maximum d'arbres
    learning_rate=learning_rate_fixed,
    max_depth=max_depth_fixed,
    validation_fraction=0.1,        # 10% pour validation
    n_iter_no_change=10,            # Arr√™t si pas d'am√©lioration pendant 10 it√©rations
    tol=1e-4,                        # Tol√©rance pour l'am√©lioration
    random_state=42
)

print("\nEntra√Ænement avec early stopping...")
start_time = time.time()
gb_early.fit(X_train, y_train)
training_time = time.time() - start_time

# Nombre d'arbres r√©ellement utilis√©s
n_estimators_used = len(gb_early.estimators_)
print(f"\nR√©sultats de l'early stopping :")
print(f"   - Temps d'entra√Ænement : {training_time:.2f} secondes")
print(f"   - Nombre d'arbres maximum : 1000")
print(f"   - Nombre d'arbres r√©ellement utilis√©s : {n_estimators_used}")
print(f"   - √âconomie : {1000 - n_estimators_used} arbres non entra√Æn√©s")

# Cr√©ation d'un jeu de validation (X_val, y_val) -- en plus du train et du test :
# Supposons que l'on a X, y (nos donn√©es compl√®tes)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# √âvaluation
y_train_pred_gb = gb_early.predict(X_train)
y_test_pred_gb = gb_early.predict(X_test)

train_acc_gb = accuracy_score(y_train, y_train_pred_gb)
test_acc_gb = accuracy_score(y_test, y_test_pred_gb)

print(f"\nPerformances :")
print(f"   - Accuracy train : {train_acc_gb:.4f} ({train_acc_gb*100:.2f}%)")
print(f"   - Accuracy test  : {test_acc_gb:.4f} ({test_acc_gb*100:.2f}%)")
print(f"   - √âcart train-test : {train_acc_gb - test_acc_gb:.4f}")

# R√©cup√©ration des scores d'entra√Ænement (loss)
train_scores = gb_early.train_score_

# Calcul des scores de validation √† chaque √©tape
val_scores = []
for y_pred in gb_early.staged_predict(X_val):
    val_scores.append(accuracy_score(y_val, y_pred))

# Visualisation de l'√©volution de la perte et de l'accuracy
plt.figure(figsize=(12, 5))

# Courbe compl√®te
plt.subplot(1, 2, 1)
plt.plot(train_scores, label='Train (loss)', linewidth=2)
plt.plot(val_scores, label='Validation (accuracy)', linewidth=2)
plt.axvline(x=n_estimators_used-1, color='red', linestyle='--',
            label=f'Arr√™t √† B={n_estimators_used}')
plt.xlabel("It√©ration (nombre d'arbres)")
plt.ylabel("Score / Loss")
plt.title("√âvolution pendant l'entra√Ænement")
plt.legend()
plt.grid(True, alpha=0.3)

# Zoom sur les derni√®res it√©rations
plt.subplot(1, 2, 2)
start_idx = max(0, n_estimators_used - 50)
plt.plot(range(start_idx, n_estimators_used),
         train_scores[start_idx:], label='Train (loss)', linewidth=2)
plt.plot(range(start_idx, n_estimators_used),
         val_scores[start_idx:], label='Validation (accuracy)', linewidth=2)
plt.axvline(x=n_estimators_used-1, color='red', linestyle='--')
plt.xlabel("It√©ration")
plt.ylabel("Score / Loss")
plt.title("Focus sur les derni√®res it√©rations")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig("gb_early_stopping.png", dpi=150)
plt.show()


# ============================================
# c) S√âLECTION D'UN "BON" ALGORITHME GRADIENT BOOSTING
# ============================================

print("\n" + "=" * 60)
print("c) OPTIMISATION DE GRADIENT BOOSTING")
print("=" * 60)

# Grille de param√®tres pour GridSearch
param_grid_gb = {
    'n_estimators': [100, 200, 300],  # Valeurs raisonnables (early stopping d√©j√† utilis√©)
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [2, 3, 4, 5],
    'subsample': [0.5, 0.7, 1.0],      # Fraction d'√©chantillons pour chaque arbre
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print(f"Grille de param√®tres √† tester :")
for param, values in param_grid_gb.items():
    print(f"   - {param} : {values}")
print(f"\nNombre total de combinaisons : {np.prod([len(v) for v in param_grid_gb.values()])}")

# Note : Avec autant de combinaisons, le GridSearch serait tr√®s long
# On va plut√¥t faire une optimisation progressive

print("\n" + "-" * 40)
print("Optimisation progressive (pour √©viter un GridSearch trop long)")

# √âtape 1 : Optimiser learning_rate et n_estimators avec max_depth fix√©
print("\n√âtape 1 : Optimisation de learning_rate et n_estimators")
print("(max_depth=3, subsample=1.0, min_samples_split=2, min_samples_leaf=1)")

learning_rates = [0.01, 0.05, 0.1, 0.2]
n_estimators_list = [50, 100, 200, 300]

results_step1 = []

for lr in learning_rates:
    for n_est in n_estimators_list:
        gb_temp = GradientBoostingClassifier(
            n_estimators=n_est,
            learning_rate=lr,
            max_depth=3,
            subsample=1.0,
            min_samples_split=2,
            min_samples_leaf=1,
            random_state=42
        )
        
        # Validation crois√©e rapide (3-fold pour gagner du temps)
        cv_scores = cross_val_score(gb_temp, X_train, y_train, cv=3)
        
        results_step1.append({
            'learning_rate': lr,
            'n_estimators': n_est,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        })
        
        print(f"  lr={lr:.2f}, B={n_est:3d} : CV={cv_scores.mean():.4f} ¬±{cv_scores.std():.4f}")

# Convertir en DataFrame
results_step1_df = pd.DataFrame(results_step1)
best_step1 = results_step1_df.loc[results_step1_df['cv_mean'].idxmax()]

print(f"\nMeilleurs param√®tres √©tape 1 :")
print(f"   - learning_rate = {best_step1['learning_rate']}")
print(f"   - n_estimators = {best_step1['n_estimators']}")
print(f"   - CV score = {best_step1['cv_mean']:.4f}")

# Visualisation
plt.figure(figsize=(10, 6))
for lr in learning_rates:
    subset = results_step1_df[results_step1_df['learning_rate'] == lr]
    plt.plot(subset['n_estimators'], subset['cv_mean'], 'o-', 
             label=f'lr={lr}', linewidth=2)

plt.xlabel('n_estimators')
plt.ylabel('Score CV moyen')
plt.title('Influence de learning_rate et n_estimators')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gb_step1_optim.png', dpi=150)
plt.show()

# √âtape 2 : Optimiser max_depth et subsample
print("\n" + "-" * 40)
print("√âtape 2 : Optimisation de max_depth et subsample")
print(f"(learning_rate={best_step1['learning_rate']}, n_estimators={best_step1['n_estimators']})")

max_depths = [2, 3, 4, 5, 6]
subsamples = [0.5, 0.7, 0.9, 1.0]

results_step2 = []

for depth in max_depths:
    for subsample in subsamples:
        gb_temp = GradientBoostingClassifier(
            n_estimators=int(best_step1['n_estimators']),
            learning_rate=best_step1['learning_rate'],
            max_depth=depth,
            subsample=subsample,
            min_samples_split=2,
            min_samples_leaf=1,
            random_state=42
        )
        
        cv_scores = cross_val_score(gb_temp, X_train, y_train, cv=3)
        
        results_step2.append({
            'max_depth': depth,
            'subsample': subsample,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        })
        
        print(f"  depth={depth}, subsample={subsample:.1f} : CV={cv_scores.mean():.4f}")

results_step2_df = pd.DataFrame(results_step2)
best_step2 = results_step2_df.loc[results_step2_df['cv_mean'].idxmax()]

print(f"\nMeilleurs param√®tres √©tape 2 :")
print(f"   - max_depth = {best_step2['max_depth']}")
print(f"   - subsample = {best_step2['subsample']}")
print(f"   - CV score = {best_step2['cv_mean']:.4f}")

# Visualisation
plt.figure(figsize=(10, 6))
for depth in max_depths:
    subset = results_step2_df[results_step2_df['max_depth'] == depth]
    plt.plot(subset['subsample'], subset['cv_mean'], 'o-', 
             label=f'depth={depth}', linewidth=2)

plt.xlabel('subsample')
plt.ylabel('Score CV moyen')
plt.title('Influence de max_depth et subsample')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('gb_step2_optim.png', dpi=150)
plt.show()

# √âtape 3 : Optimiser min_samples_split et min_samples_leaf
print("\n" + "-" * 40)
print("√âtape 3 : Optimisation de min_samples_split et min_samples_leaf")
print(f"(learning_rate={best_step1['learning_rate']}, n_estimators={best_step1['n_estimators']}, "
      f"max_depth={best_step2['max_depth']}, subsample={best_step2['subsample']})")

min_samples_splits = [2, 5, 10, 20]
min_samples_leafs = [1, 2, 5, 10]

results_step3 = []

for min_split in min_samples_splits:
    for min_leaf in min_samples_leafs:
        gb_temp = GradientBoostingClassifier(
            n_estimators=int(best_step1['n_estimators']),
            learning_rate=best_step1['learning_rate'],
            max_depth=int(best_step2['max_depth']),
            subsample=best_step2['subsample'],
            min_samples_split=min_split,
            min_samples_leaf=min_leaf,
            random_state=42
        )
        
        cv_scores = cross_val_score(gb_temp, X_train, y_train, cv=3)
        
        results_step3.append({
            'min_samples_split': min_split,
            'min_samples_leaf': min_leaf,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        })
        
        print(f"  split={min_split:2d}, leaf={min_leaf:2d} : CV={cv_scores.mean():.4f}")

results_step3_df = pd.DataFrame(results_step3)
best_step3 = results_step3_df.loc[results_step3_df['cv_mean'].idxmax()]

print(f"\nMeilleurs param√®tres √©tape 3 :")
print(f"   - min_samples_split = {best_step3['min_samples_split']}")
print(f"   - min_samples_leaf = {best_step3['min_samples_leaf']}")
print(f"   - CV score = {best_step3['cv_mean']:.4f}")

# ============================================
# CONSTRUCTION DU GRADIENT BOOSTING OPTIMIS√â
# ============================================

print("\n" + "=" * 60)
print("CONSTRUCTION DU GRADIENT BOOSTING OPTIMIS√â")
print("=" * 60)

# Rassembler tous les meilleurs param√®tres
best_params_gb = {
    'n_estimators': int(best_step1['n_estimators']),
    'learning_rate': best_step1['learning_rate'],
    'max_depth': int(best_step2['max_depth']),
    'subsample': best_step2['subsample'],
    'min_samples_split': int(best_step3['min_samples_split']),
    'min_samples_leaf': int(best_step3['min_samples_leaf']),
    'validation_fraction': 0.1,
    'n_iter_no_change': 10,
    'random_state': 42
}

print(f"\nMeilleurs param√®tres trouv√©s :")
for param, value in best_params_gb.items():
    print(f"   - {param} : {value}")

# Entra√Æner le mod√®le final
gb_optimized = GradientBoostingClassifier(**best_params_gb)

print("\nEntra√Ænement du mod√®le final...")
start_time = time.time()
gb_optimized.fit(X_train, y_train)
training_time = time.time() - start_time

print(f"‚úì Entra√Ænement termin√© en {training_time:.2f} secondes")
print(f"   - Nombre d'arbres r√©el : {len(gb_optimized.estimators_)}")

# √âvaluation
y_train_pred_gb_opt = gb_optimized.predict(X_train)
y_test_pred_gb_opt = gb_optimized.predict(X_test)

train_acc_gb_opt = accuracy_score(y_train, y_train_pred_gb_opt)
test_acc_gb_opt = accuracy_score(y_test, y_test_pred_gb_opt)

print(f"\nPerformances du Gradient Boosting optimis√© :")
print(f"   - Accuracy train : {train_acc_gb_opt:.4f} ({train_acc_gb_opt*100:.2f}%)")
print(f"   - Accuracy test  : {test_acc_gb_opt:.4f} ({test_acc_gb_opt*100:.2f}%)")

# ============================================
# COMPARAISON AVEC LES MOD√àLES PR√âC√âDENTS
# ============================================

print("\n" + "=" * 60)
print("COMPARAISON AVEC ARBRE OPTIMIS√â ET RANDOM FOREST")
print("=" * 60)

comparison_ensemble = pd.DataFrame({
    'Mod√®le': [
        'Arbre optimis√©',
        'Random Forest optimis√©e',
        'Gradient Boosting optimis√©'
    ],
    'Accuracy test': [
        test_score_best,
        test_acc_rf_opt,
        test_acc_gb_opt
    ]
})

print("\nTableau comparatif :")
print(comparison_ensemble.to_string(index=False))

# Visualisation
plt.figure(figsize=(8, 5))
bars = plt.bar(comparison_ensemble['Mod√®le'], comparison_ensemble['Accuracy test'],
               color=['lightblue', 'orange', 'green'])
plt.xlabel('Mod√®les')
plt.ylabel('Accuracy')
plt.title('Comparaison Arbre vs Random Forest vs Gradient Boosting')
plt.ylim([0.7, 0.85])  # Zoom sur la plage pertinente

# Ajouter les valeurs
for bar, val in zip(bars, comparison_ensemble['Accuracy test']):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
             f'{val:.2%}', ha='center', va='bottom')

plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('comparison_tree_rf_gb.png', dpi=150)
plt.show()

# Matrice de confusion pour Gradient Boosting
cm_gb = confusion_matrix(y_test, y_test_pred_gb_opt)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_gb, annot=True, fmt='d', cmap='Greens',
            xticklabels=['‚â§50K', '>50K'],
            yticklabels=['‚â§50K', '>50K'])
plt.xlabel('Pr√©dit')
plt.ylabel('R√©el')
plt.title(f'Matrice de confusion - Gradient Boosting optimis√©\nAccuracy: {test_acc_gb_opt:.2%}')
plt.tight_layout()
plt.savefig('confusion_matrix_gb.png', dpi=150)
plt.show()

# ============================================
# ANALYSE DES R√âSULTATS
# ============================================

print("\n" + "=" * 60)
print("ANALYSE DES R√âSULTATS")
print("=" * 60)

# D√©terminer le meilleur mod√®le
best_model_name = comparison_ensemble.loc[comparison_ensemble['Accuracy test'].idxmax(), 'Mod√®le']
best_model_score = comparison_ensemble['Accuracy test'].max()

print(f"\nMeilleur mod√®le : {best_model_name}")
print(f"   - Accuracy test : {best_model_score:.2%}")

print("""
Interpr√©tation des r√©sultats :

1. Arbre de d√©cision optimis√© :
   - Performance de base
   - Simple et interpr√©table
   - Limit√© par sa structure unique

2. Random Forest :
   - Am√©liore l'arbre gr√¢ce au bagging
   - R√©duit la variance
   - Performance g√©n√©ralement meilleure

3. Gradient Boosting :
   - Approche s√©quentielle
   - Corrige les erreurs progressivement
   - Souvent le meilleur sur les donn√©es tabulaires
""")

# V√©rifier si le boosting surpasse les autres
if best_model_name == 'Gradient Boosting optimis√©':
    improvement_vs_rf = (test_acc_gb_opt - test_acc_rf_opt) / test_acc_rf_opt * 100
    print(f"\nLe Gradient Boosting surpasse la Random Forest de {improvement_vs_rf:.1f}%")
    
    if test_acc_gb_opt > 0.8:
        print("‚úì Excellent score (>80%) - Le mod√®le est tr√®s performant")
    elif test_acc_gb_opt > 0.75:
        print("‚úì Bon score (>75%) - Le mod√®le est satisfaisant")
    else:
        print("‚ö† Score modeste - Peut-√™tre besoin de plus de features ou de tuning")

# Sauvegarder le mod√®le
with open('gradient_boosting_optimized.pkl', 'wb') as f:
    pickle.dump(gb_optimized, f)
print("\n‚úì Gradient Boosting optimis√© sauvegard√© sous 'gradient_boosting_optimized.pkl'")

# ============================================
# CONCLUSION DE LA PARTIE 1-D
# ============================================

print("\n" + "=" * 60)
print("CONCLUSIONS DE LA PARTIE 1-D")
print("=" * 60)

print("""
Points cl√©s √† retenir sur le Boosting :

1. Param√®tres cruciaux :
   - learning_rate et n_estimators sont li√©s (compromis)
   - max_depth doit rester faible (arbres peu profonds)
   - subsample ajoute de l'al√©atoire (comme RF)

2. Early stopping :
   - √âvite le sur-apprentissage
   - √âconomise du temps de calcul
   - validation_fraction et n_iter_no_change sont essentiels

3. Performance :
   - Le Gradient Boosting est souvent le meilleur sur donn√©es tabulaires
   - Mais plus lent √† l'entra√Ænement que Random Forest
   - Sensible au sur-apprentissage si mal param√©tr√©

4. Prochaine √©tape :
   - Analyser l'importance des variables
   - Tracer les courbes ROC
""")


# ============================================
# PARTIE 1-E : S√âLECTION DE VARIABLES (FEATURE IMPORTANCE)
# ============================================

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print("=" * 60)
print("PARTIE 1-E : S√âLECTION DE VARIABLES (FEATURE IMPORTANCE)")
print("=" * 60)

print("""
L'importance des variables (feature importance) :
- Mesure la contribution de chaque variable √† la pr√©diction
- Pour les arbres : bas√©e sur la r√©duction de l'impuret√©
- Pour les m√©thodes d'ensemble : moyenne sur tous les arbres
- Permet d'interpr√©ter le mod√®le et de faire de la s√©lection de features
""")

# R√©cup√©rer les noms des features
feature_names = X_train.columns.tolist()

# ============================================
# 1. IMPORTANCE DES VARIABLES POUR L'ARBRE OPTIMIS√â
# ============================================

print("\n" + "=" * 40)
print("1. ARBRE OPTIMIS√â - FEATURE IMPORTANCE")
print("=" * 40)

# R√©cup√©rer l'arbre optimis√© (de la partie 1-B)
tree_importance = best_tree.feature_importances_

# Cr√©er un DataFrame pour trier
tree_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': tree_importance
}).sort_values('importance', ascending=False)

print("\nTop 10 features les plus importantes (Arbre) :")
print(tree_importance_df.head(10).to_string(index=False))

# Visualisation
plt.figure(figsize=(12, 8))
plt.subplot(2, 2, 1)
plt.barh(tree_importance_df.head(15)['feature'][::-1], 
         tree_importance_df.head(15)['importance'][::-1])
plt.xlabel('Importance')
plt.title('Arbre de d√©cision - Top 15 features')
plt.tight_layout()

# ============================================
# 2. IMPORTANCE POUR LA RANDOM FOREST
# ============================================

print("\n" + "=" * 40)
print("2. RANDOM FOREST - FEATURE IMPORTANCE")
print("=" * 40)

rf_importance = rf_optimized.feature_importances_

rf_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_importance
}).sort_values('importance', ascending=False)

print("\nTop 10 features les plus importantes (Random Forest) :")
print(rf_importance_df.head(10).to_string(index=False))

# Visualisation
plt.subplot(2, 2, 2)
plt.barh(rf_importance_df.head(15)['feature'][::-1], 
         rf_importance_df.head(15)['importance'][::-1], color='orange')
plt.xlabel('Importance')
plt.title('Random Forest - Top 15 features')
plt.tight_layout()

# ============================================
# 3. IMPORTANCE POUR LE GRADIENT BOOSTING
# ============================================

print("\n" + "=" * 40)
print("3. GRADIENT BOOSTING - FEATURE IMPORTANCE")
print("=" * 40)

gb_importance = gb_optimized.feature_importances_

gb_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': gb_importance
}).sort_values('importance', ascending=False)

print("\nTop 10 features les plus importantes (Gradient Boosting) :")
print(gb_importance_df.head(10).to_string(index=False))

# Visualisation
plt.subplot(2, 2, 3)
plt.barh(gb_importance_df.head(15)['feature'][::-1], 
         gb_importance_df.head(15)['importance'][::-1], color='green')
plt.xlabel('Importance')
plt.title('Gradient Boosting - Top 15 features')
plt.tight_layout()

# ============================================
# 4. COMPARAISON DES IMPORTANCES
# ============================================

print("\n" + "=" * 40)
print("4. COMPARAISON DES TROIS MOD√àLES")
print("=" * 40)

# Fusionner les trois DataFrames
comparison_importance = pd.DataFrame({
    'feature': feature_names,
    'Arbre': tree_importance,
    'Random_Forest': rf_importance,
    'Gradient_Boosting': gb_importance
})

# Normaliser pour que la somme = 1 pour chaque mod√®le
for col in ['Arbre', 'Random_Forest', 'Gradient_Boosting']:
    comparison_importance[col] = comparison_importance[col] / comparison_importance[col].sum()

# Top 10 features communes
top_features_tree = set(tree_importance_df.head(10)['feature'])
top_features_rf = set(rf_importance_df.head(10)['feature'])
top_features_gb = set(gb_importance_df.head(10)['feature'])

common_features = top_features_tree & top_features_rf & top_features_gb
print(f"\nFeatures communes dans les top 10 des trois mod√®les :")
for f in common_features:
    print(f"   - {f}")

# Visualisation comparative
plt.subplot(2, 2, 4)
# Prendre les 10 features les plus importantes en moyenne
mean_importance = comparison_importance[['Arbre', 'Random_Forest', 'Gradient_Boosting']].mean(axis=1)
comparison_importance['mean'] = mean_importance
top10_mean = comparison_importance.nlargest(10, 'mean')['feature'].values

# Pr√©parer les donn√©es pour le graphique
plot_data = comparison_importance[comparison_importance['feature'].isin(top10_mean)]
plot_data = plot_data.set_index('feature')
plot_data[['Arbre', 'Random_Forest', 'Gradient_Boosting']].plot(kind='bar', ax=plt.gca())
plt.title('Comparaison des importances - Top 10 features')
plt.xlabel('Features')
plt.ylabel('Importance (normalis√©e)')
plt.xticks(rotation=45, ha='right')
plt.legend()

plt.tight_layout()
plt.savefig('feature_importance_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

# ============================================
# 5. EXPLICATION DU CALCUL DES IMPORTANCES
# ============================================

print("\n" + "=" * 40)
print("5. COMMENT SONT CALCUL√âES CES IMPORTANCES ?")
print("=" * 40)

print("""
Pour les arbres de d√©cision (et par extension Random Forest, Gradient Boosting) :

1. Importance bas√©e sur l'impuret√© (MDI - Mean Decrease in Impurity) :
   - √Ä chaque split d'un noeud, on mesure la r√©duction de l'impuret√© (Gini ou Entropie)
   - Cette r√©duction est pond√©r√©e par le nombre d'√©chantillons concern√©s
   - On somme ces r√©ductions pour chaque feature sur tous les splits de tous les arbres
   - On normalise pour que la somme totale = 1

2. Pour Random Forest :
   - On moyenne les importances de tous les arbres
   - Plus robuste qu'un seul arbre

3. Pour Gradient Boosting :
   - Principe similaire, mais les arbres sont pond√©r√©s par leur learning_rate
   - Les premiers arbres ont plus d'influence que les suivants

4. Limites :
   - Favorise les variables num√©riques avec beaucoup de modalit√©s
   - Ne capture pas les interactions complexes
   - Peut √™tre biais√© si les features sont corr√©l√©es
""")

# ============================================
# 6. PERMUTATION IMPORTANCE (M√âTHODE ALTERNATIVE)
# ============================================

print("\n" + "=" * 40)
print("6. PERMUTATION IMPORTANCE (M√âTHODE PLUS ROBUSTE)")
print("=" * 40)

print("""
La permutation importance :
- Principe : on permute al√©atoirement une feature et on observe la baisse de performance
- Si la permutation fait chuter le score ‚Üí feature importante
- Plus fiable que l'importance bas√©e sur l'impuret√©, surtout pour features corr√©l√©es
""")

# Calculer la permutation importance pour la Random Forest (comme exemple)
print("\nCalcul de la permutation importance pour Random Forest (peut prendre du temps)...")
perm_importance = permutation_importance(
    rf_optimized, X_test, y_test,
    n_repeats=10,      # Nombre de permutations pour chaque feature
    random_state=42,
    n_jobs=-1
)

# Cr√©er un DataFrame
perm_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values('importance_mean', ascending=False)

print("\nTop 10 features par permutation importance :")
print(perm_importance_df.head(10).to_string(index=False))

# Comparer avec l'importance standard
comparison_methods = pd.merge(
    rf_importance_df.head(10),
    perm_importance_df.head(10),
    on='feature',
    how='outer'
)
print("\nComparaison des deux m√©thodes (top 10) :")
print(comparison_methods.to_string(index=False))

# Visualisation
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.barh(rf_importance_df.head(10)['feature'][::-1], 
         rf_importance_df.head(10)['importance'][::-1], color='orange')
plt.xlabel('Importance (MDI)')
plt.title('Random Forest - Importance standard')

plt.subplot(1, 2, 2)
plt.barh(perm_importance_df.head(10)['feature'][::-1], 
         perm_importance_df.head(10)['importance_mean'][::-1],
         xerr=perm_importance_df.head(10)['importance_std'][::-1],
         color='purple', capsize=3)
plt.xlabel('Importance (Permutation)')
plt.title('Random Forest - Permutation importance')

plt.tight_layout()
plt.savefig('permutation_vs_standard.png', dpi=150)
plt.show()

# ============================================
# PARTIE 1-F : COURBES ROC
# ============================================

from sklearn.metrics import roc_curve, auc, roc_auc_score

print("\n" + "=" * 60)
print("PARTIE 1-F : COURBES ROC")
print("=" * 60)

print("""
Courbe ROC (Receiver Operating Characteristic) :
- Repr√©sente le taux de vrais positifs (TPR) en fonction du taux de faux positifs (FPR)
- Permet d'√©valuer la qualit√© du score (pas seulement de la classe pr√©dite)
- Plus l'AUC (Area Under Curve) est proche de 1, meilleur est le mod√®le
- AUC = 0.5 : mod√®le al√©atoire
- AUC = 1.0 : mod√®le parfait
""")

# ============================================
# 1. COMMENT PR√âDIRE UN SCORE AVEC RF ET GB ?
# ============================================

print("\n" + "=" * 40)
print("1. PR√âDICTION DE SCORES")
print("=" * 40)

# Pour Random Forest : proba d'appartenir √† la classe 1 (>50K)
rf_proba = rf_optimized.predict_proba(X_test)[:, 1]
print("\nRandom Forest - Scores de probabilit√© :")
print(f"   - Shape des probas : {rf_proba.shape}")
print(f"   - Min : {rf_proba.min():.4f}")
print(f"   - Max : {rf_proba.max():.4f}")
print(f"   - Moyenne : {rf_proba.mean():.4f}")

# Pour Gradient Boosting
gb_proba = gb_optimized.predict_proba(X_test)[:, 1]
print("\nGradient Boosting - Scores de probabilit√© :")
print(f"   - Shape des probas : {gb_proba.shape}")
print(f"   - Min : {gb_proba.min():.4f}")
print(f"   - Max : {gb_proba.max():.4f}")
print(f"   - Moyenne : {gb_proba.mean():.4f}")

# Pour l'arbre
tree_proba = best_tree.predict_proba(X_test)[:, 1]
print("\nArbre optimis√© - Scores de probabilit√© :")
print(f"   - Shape des probas : {tree_proba.shape}")
print(f"   - Min : {tree_proba.min():.4f}")
print(f"   - Max : {tree_proba.max():.4f}")
print(f"   - Moyenne : {tree_proba.mean():.4f}")

# ============================================
# 2. TRAC√â DES COURBES ROC
# ============================================

print("\n" + "=" * 40)
print("2. TRAC√â DES COURBES ROC")
print("=" * 40)

# Calculer les points des courbes ROC
fpr_tree, tpr_tree, thresholds_tree = roc_curve(y_test, tree_proba)
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_proba)
fpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, gb_proba)

# Calculer les AUC
auc_tree = auc(fpr_tree, tpr_tree)
auc_rf = auc(fpr_rf, tpr_rf)
auc_gb = auc(fpr_gb, tpr_gb)

print(f"\nAUC (Area Under Curve) :")
print(f"   - Arbre optimis√© : {auc_tree:.4f}")
print(f"   - Random Forest : {auc_rf:.4f}")
print(f"   - Gradient Boosting : {auc_gb:.4f}")

# Tracer les courbes ROC
plt.figure(figsize=(10, 8))

# Courbes
plt.plot(fpr_tree, tpr_tree, 'b-', linewidth=2, 
         label=f'Arbre (AUC = {auc_tree:.3f})')
plt.plot(fpr_rf, tpr_rf, 'orange', linewidth=2, 
         label=f'Random Forest (AUC = {auc_rf:.3f})')
plt.plot(fpr_gb, tpr_gb, 'g-', linewidth=2, 
         label=f'Gradient Boosting (AUC = {auc_gb:.3f})')

# Diagonale (mod√®le al√©atoire)
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Al√©atoire (AUC = 0.5)')

# Personnalisation
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)
plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)
plt.title('Courbes ROC - Comparaison des mod√®les', fontsize=14)
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)

# Ajouter un zoom sur le coin sup√©rieur gauche (optionnel)
plt.text(0.6, 0.2, f'Meilleur mod√®le : {best_model_name}', 
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.savefig('roc_curves_comparison.png', dpi=150)
plt.show()

# ============================================
# 3. ANALYSE DES SEUILS
# ============================================

print("\n" + "=" * 40)
print("3. ANALYSE DES SEUILS DE D√âCISION")
print("=" * 40)

# Pour le meilleur mod√®le (Gradient Boosting g√©n√©ralement)
print("\nAnalyse pour Gradient Boosting (meilleur AUC) :")

# Cr√©er un DataFrame avec les seuils et m√©triques
thresholds_df = pd.DataFrame({
    'threshold': thresholds_gb,
    'fpr': fpr_gb,
    'tpr': tpr_gb,
    'youden_j': tpr_gb - fpr_gb  # Indice de Youden (maximise tpr - fpr)
})

# Enlever la derni√®re ligne (threshold = infini)
thresholds_df = thresholds_df[:-1]

# Meilleur seuil selon Youden
best_threshold_idx = thresholds_df['youden_j'].idxmax()
best_threshold = thresholds_df.loc[best_threshold_idx, 'threshold']
best_youden = thresholds_df.loc[best_threshold_idx, 'youden_j']

print(f"\nMeilleur seuil (maximisant tpr - fpr) : {best_threshold:.4f}")
print(f"   - TPR √† ce seuil : {thresholds_df.loc[best_threshold_idx, 'tpr']:.4f}")
print(f"   - FPR √† ce seuil : {thresholds_df.loc[best_threshold_idx, 'fpr']:.4f}")
print(f"   - Indice de Youden : {best_youden:.4f}")

# Comparer avec le seuil par d√©faut (0.5)
default_idx = (thresholds_df['threshold'] - 0.5).abs().idxmin()
print(f"\nSeuil par d√©faut (0.5) :")
print(f"   - TPR : {thresholds_df.loc[default_idx, 'tpr']:.4f}")
print(f"   - FPR : {thresholds_df.loc[default_idx, 'fpr']:.4f}")

# Visualisation de l'√©volution
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(thresholds_df['threshold'], thresholds_df['tpr'], 'g-', label='TPR', linewidth=2)
plt.plot(thresholds_df['threshold'], thresholds_df['fpr'], 'r-', label='FPR', linewidth=2)
plt.axvline(x=best_threshold, color='blue', linestyle='--', 
            label=f'Meilleur seuil ({best_threshold:.2f})')
plt.axvline(x=0.5, color='black', linestyle=':', label='Seuil d√©faut (0.5)')
plt.xlabel('Seuil de d√©cision')
plt.ylabel('Taux')
plt.title('TPR et FPR en fonction du seuil')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(thresholds_df['threshold'], thresholds_df['youden_j'], 'purple', linewidth=2)
plt.axvline(x=best_threshold, color='blue', linestyle='--', 
            label=f'Max Youden = {best_youden:.3f}')
plt.xlabel('Seuil de d√©cision')
plt.ylabel('Indice de Youden (TPR - FPR)')
plt.title('Optimisation du seuil')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('threshold_analysis.png', dpi=150)
plt.show()

# ============================================
# 4. INTERPR√âTATION DES COURBES ROC
# ============================================

print("\n" + "=" * 40)
print("4. INTERPR√âTATION DES R√âSULTATS")
print("=" * 40)

print(f"""
Interpr√©tation des courbes ROC :

1. Classement des mod√®les par AUC :
   - Gradient Boosting : {auc_gb:.4f}
   - Random Forest : {auc_rf:.4f}
   - Arbre optimis√© : {auc_tree:.4f}

2. Signification de l'AUC :
   - AUC = 0.5 : mod√®le al√©atoire
   - AUC = 0.7-0.8 : acceptable
   - AUC = 0.8-0.9 : excellent
   - AUC = 0.9-1.0 : exceptionnel

3. Notre meilleur mod√®le ({best_model_name}) :
   - AUC = {auc_gb:.4f} ‚Üí {'Excellent' if auc_gb > 0.8 else 'Bon' if auc_gb > 0.7 else 'Moyen'}

4. Compromis TPR/FPR :
   - En baissant le seuil, on augmente TPR mais aussi FPR
   - Le choix du seuil d√©pend du co√ªt des erreurs :
     * Si on veut absolument d√©tecter les >50K (TPR √©lev√©) ‚Üí seuil bas
     * Si on veut √©viter les faux positifs (FPR bas) ‚Üí seuil haut
""")

# ============================================
# 5. SAUVEGARDE DES MOD√àLES FINAUX
# ============================================

print("\n" + "=" * 40)
print("5. SAUVEGARDE DES MOD√àLES FINAUX")
print("=" * 40)

# Sauvegarder tous les mod√®les importants
models_to_save = {
    'best_tree': best_tree,
    'random_forest': rf_optimized,
    'gradient_boosting': gb_optimized,
    'scaler': scaler
}

for name, model in models_to_save.items():
    with open(f'{name}.pkl', 'wb') as f:
        pickle.dump(model, f)
    print(f"‚úì {name}.pkl sauvegard√©")

# Sauvegarder aussi les m√©triques pour le rapport
metrics = {
    'tree_accuracy': test_score_best,
    'tree_auc': auc_tree,
    'rf_accuracy': test_acc_rf_opt,
    'rf_auc': auc_rf,
    'gb_accuracy': test_acc_gb_opt,
    'gb_auc': auc_gb,
    'best_model': best_model_name,
    'best_threshold': best_threshold
}

with open('final_metrics.pkl', 'wb') as f:
    pickle.dump(metrics, f)
print("‚úì M√©triques finales sauvegard√©es")

# ============================================
# CONCLUSION DES PARTIES 1-E et 1-F
# ============================================

print("\n" + "=" * 60)
print("CONCLUSION DES PARTIES 1-E ET 1-F")
print("=" * 60)

print("""
Points cl√©s √† retenir :

1. Feature Importance :
   - Les trois mod√®les s'accordent sur les variables cl√©s
   - Professional, Income, Unemployment sont d√©terminants
   - La permutation importance confirme ces r√©sultats

2. Courbes ROC :
   - Le Gradient Boosting est le meilleur (AUC le plus √©lev√©)
   - Tous les mod√®les battent largement le hasard (AUC > 0.5)
   - Les courbes permettent de choisir un seuil adapt√©

3. Pour le d√©ploiement :
   - On garde le Gradient Boosting comme mod√®le final
   - Avec un seuil optimis√© ({best_threshold:.3f}) si n√©cessaire
   - Les mod√®les sont sauvegard√©s pour Streamlit
""")


